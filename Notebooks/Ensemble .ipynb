{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "import re\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_absolute_errors(submission_df, comparison_df):\n",
    "    \"\"\"\n",
    "    This function takes a submission entry for public leaderboard, and returns\n",
    "    the training error for each month.\n",
    "    \"\"\"\n",
    "    # training error\n",
    "    trainresults = pd.merge(submission_df[['ParcelId','201610','201611','201612']], comparison_df[['parcelid','logerror','month']],\n",
    "                           left_on='ParcelId', right_on='parcelid')\n",
    "    oct_error = abs(trainresults[trainresults['month'] == 10]['201610'] \n",
    "                    - trainresults[trainresults['month'] == 10]['logerror']).mean()\n",
    "    nov_error = abs(trainresults[trainresults['month'] == 11]['201611'] \n",
    "                    - trainresults[trainresults['month'] == 11]['logerror']).mean()\n",
    "    dec_error = abs(trainresults[trainresults['month'] == 12]['201612'] \n",
    "                    - trainresults[trainresults['month'] == 12]['logerror']).mean()\n",
    "    overall_months_mae = (oct_error*(trainresults['month'] == 10).sum() + nov_error*(trainresults['month'] == 11).sum() \n",
    "                        + dec_error*(trainresults['month'] == 12).sum()) / (trainresults['month'].isin([10,11,12])).sum()\n",
    "    \n",
    "    overall_mae = abs(trainresults['201612'] - trainresults['logerror']).mean()\n",
    "    return (oct_error, nov_error, dec_error, overall_months_mae, overall_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')\n",
    "del logerror, properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in predictions from the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/anerdi/Desktop/Zillow/submissions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# available_base_learners = {'{0}_{1}'.format(re.search(r'stage1_([a-z,0-9_]+)_stage2',f).group(1)\n",
    "#                     ,re.search(r'stage2_([a-z,0-9_]+)_',f).group(1)): f\n",
    "#           for f in os.listdir(submission_dir) if re.match(r'^    two_stage_stage1_[a-z,0-9,_]+',f) is not None}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annrfsxgbs_LME = pd.read_csv(\"two_stage_stage1_stacked_annrfsxgbs_stage2_lme.csv.gz\", compression=\"gzip\")\n",
    "annrfs_LME = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage_lme.csv.gz\", compression=\"gzip\")\n",
    "rf_lme = pd.read_csv(\"two_stage_stage1_rf_stage2_lme.csv.gz\", compression=\"gzip\")\n",
    "logistic_LME = pd.read_csv(\"two_stage_lme.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "RF_2 = pd.read_csv(\"RF_n100_maxfeat10_maxdepth20_extreme.gz\", compression=\"gzip\")\n",
    "rf_overfit = pd.read_csv(\"    two_stage_stage1_stacked_annrfsxgbs_stage2_rf_maxdepth12_age.csv.gz\")\n",
    "\n",
    "XGB = pd.read_csv(\"two_stage_xgb.csv.gz\", compression=\"gzip\")\n",
    "XGB600 = pd.read_csv(\"XGB_600.gz\", compression=\"gzip\")\n",
    "XGB3000 = pd.read_csv(\"XGB_3000_RF.gz\", compression='gzip')\n",
    "\n",
    "stacked_final = pd.read_csv(\"super_learner_preds.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [(stacked_final,0.80),\n",
    "    (annrfsxgbs_LME, 0.09),(annrfs_LME,0.06),\n",
    "    (XGB3000, 0.03),\n",
    "    (RF_2, 0.02)]\n",
    "\n",
    "sum([y for x,y in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = stacked_final[['ParcelId']].copy()\n",
    "cols = ['201610','201611','201612','201710','201711','201712']\n",
    "foo = models[0][0][cols]*models[0][1]\n",
    "for pair in models[1:]:\n",
    "    model,wt = pair\n",
    "    foo = foo + model[cols]*wt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06132045408880847,\n",
       " 0.06046500547645126,\n",
       " 0.07294134560092015,\n",
       " 0.063503394989463821,\n",
       " 0.06627135751869448)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = pd.concat([ensemble,foo.round(4)], axis=1)\n",
    "ensemble['ParcelId'] = ensemble['ParcelId'].astype(int)\n",
    "assert all(ensemble.ParcelId.unique() == stacked_final.ParcelId.unique())\n",
    "mean_absolute_errors(ensemble, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06132716495881037,\n",
       " 0.06047294633077763,\n",
       " 0.0729488211615872,\n",
       " 0.063510524467337778,\n",
       " 0.06627859207975909)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best\n",
    "ensemble = pd.concat([ensemble,foo.round(4)], axis=1)\n",
    "ensemble['ParcelId'] = ensemble['ParcelId'].astype(int)\n",
    "assert all(ensemble.ParcelId.unique() == stacked_final.ParcelId.unique())\n",
    "mean_absolute_errors(ensemble, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble.to_csv(\"new_ensemble.gz\", index=False, float_format='%.4g', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
