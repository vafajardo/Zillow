{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import feature_pipelines as pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anerdi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proportion of living area\n",
    "properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet']/properties['lotsizesquarefeet']\n",
    "\n",
    "#Ratio of the built structure value to land area\n",
    "properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt']/properties['landtaxvaluedollarcnt']\n",
    "\n",
    "#Ratio of tax of property over parcel\n",
    "properties['N-ValueRatio'] = properties['taxvaluedollarcnt']/properties['taxamount']\n",
    "\n",
    "# Pool\n",
    "properties['Pool'] = (properties['pooltypeid2'].fillna(0) + properties['pooltypeid7'].fillna(0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')\n",
    "data['wts_oct'] = np.where(data['month'] == 10, 1.5, 1)\n",
    "data['wts_nov'] = np.where(data['month'] == 11, 1.5, 1)\n",
    "data['wts_dec'] = np.where(data['month'] == 12, 1.5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup variables considered in the model\n",
    "\n",
    "# numerical variables\n",
    "# num_atts = ['calculatedfinishedsquarefeet','bathroomcnt','structuretaxvaluedollarcnt',\n",
    "#              'bedroomcnt','calculatedbathnbr','roomcnt','longitude','threequarterbathnbr', 'yardbuildingsqft17',\n",
    "#              'numberofstories','N-ValueRatio','N-ValueProp','N-LivingAreaProp']\n",
    "\n",
    "num_atts = ['calculatedfinishedsquarefeet','bathroomcnt','structuretaxvaluedollarcnt',\n",
    "             'bedroomcnt','calculatedbathnbr','N-ValueRatio','N-ValueProp','N-LivingAreaProp']\n",
    "\n",
    "# categorical varaibles\n",
    "cat_atts = ['airconditioningtypeid','heatingorsystemtypeid','Pool','propertylandusetypeid','taxdelinquencyflag',\n",
    "           'regionidcounty']\n",
    "\n",
    "# Dictionary of categorical variables and their default levels\n",
    "cat_dict = {key:value for key,value in {'airconditioningtypeid':[-1] + list(range(1,14)),\n",
    "           'architecturalstyletypeid':[-1] + list(range(1,28)),\n",
    "           'buildingclasstypeid':[-1] + list(range(1,6)),\n",
    "            'heatingorsystemtypeid':[-1] + list(range(1,26)),\n",
    "            'pooltypeid10': list(range(-1,2)),\n",
    "            'pooltypeid2': list(range(-1,2)),\n",
    "            'pooltypeid7': list(range(-1,2)),\n",
    "            'Pool': [0,1],\n",
    "            'propertylandusetypeid': [-1, 31,46,47,246,247,248,260,261,262,263,264,265,266,267,268,269,270,271,\n",
    "                                     273,274,275,276,279,290,291],\n",
    "            'regionidcounty': [2061,3101,1286],\n",
    "            'storytypeid':[-1] + list(range(1,36)),\n",
    "            'typeconstructiontypeid':[-1] + list(range(1,19)),\n",
    "            'yearbuilt': [-1] + list(range(1885,2018)),\n",
    "            'fireplaceflag': [-1] + ['True','False'],\n",
    "            'taxdelinquencyflag': [-1] + ['Y','N']\n",
    "           }.items() if key in cat_atts}\n",
    "\n",
    "# pairs to interact (x1,x2) where x1 is categorical and x2 is continuous\n",
    "interact_pairs = [('regionidcounty','bathroomcnt'),('regionidcounty','bedroomcnt')\n",
    "                 ,('regionidcounty','structuretaxvaluedollarcnt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "        ('select_and_dummify', pipes.DF_Selector_GetDummies(cat_dict)),\n",
    "    ])\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', pipes.DataFrameSelector(num_atts)),\n",
    "        ('imputer', Imputer()),\n",
    "    ])\n",
    "\n",
    "# interaction pipeline\n",
    "interact_pipeline = Pipeline([\n",
    "        ('dummify_and_interact',pipes.Dummify_and_Interact(interact_pairs,cat_dict)),\n",
    "    ])\n",
    "\n",
    "# Full pipeline\n",
    "feature_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "        (\"interact_pipeline\", interact_pipeline)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into the 10-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2199, 86155, 84691, ..., 86952, 82677, 76398])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(19)\n",
    "np.random.shuffle(indices) # in-place shuffling \n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_indices = {(i+1):indices[i::10] for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 2199, 83721, 29492, ..., 37852, 48911, 39220]),\n",
       " 2: array([86155, 32252, 81949, ..., 57319, 13479, 33811]),\n",
       " 3: array([84691, 37597,  3215, ..., 84821, 43372, 86952]),\n",
       " 4: array([11172, 67082, 58364, ..., 74500, 63830, 82677]),\n",
       " 5: array([78769, 73075, 17232, ..., 12489,   266, 76398]),\n",
       " 6: array([53035, 17238, 32604, ..., 14649, 26827, 61025]),\n",
       " 7: array([58194, 72307,  3380, ..., 57397, 68361, 53125]),\n",
       " 8: array([66378, 81551, 66156, ..., 73922, 85799, 45218]),\n",
       " 9: array([70318, 70507, 20646, ...,  7537, 69584, 17218]),\n",
       " 10: array([42552, 66817, 57336, ..., 88913, 67815, 17738])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stage 1 estimated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_rfs_probabilities = pd.read_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs_stacked_rfs.csv.gz\")\n",
    "stacked_rfs_probabilities.rename(columns={'stacked_pred':\"overestimate_prob\"}, inplace=True)\n",
    "stacked_rfs_probabilities = pd.merge(data[['parcelid']], stacked_rfs_probabilities, on='parcelid')\n",
    "\n",
    "stacked_annrfs_probabilities = pd.read_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs_stacked_ann_rfs.csv.gz\")\n",
    "stacked_annrfs_probabilities.rename(columns={'stacked_pred':\"overestimate_prob\"}, inplace=True)\n",
    "stacked_annrfs_probabilities = pd.merge(data[['parcelid']], stacked_annrfs_probabilities, on='parcelid')\n",
    "\n",
    "logistic_probabiliies = pd.read_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs.csv.gz\")\n",
    "logistic_probabiliies = pd.merge(data[['parcelid']], logistic_probabiliies, on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (stacked_rfs_probabilities.parcelid == data.parcelid).all()\n",
    "assert (stacked_annrfs_probabilities.parcelid == data.parcelid).all()\n",
    "assert (logistic_probabiliies.parcelid == data.parcelid).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage1_models = [\n",
    "    ('stacked_rfs', stacked_rfs_probabilities),\n",
    "    ('stacked_annrfs', stacked_annrfs_probabilities),\n",
    "    ('logistic', logistic_probabiliies)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models on the 10 splits of data \\ fold_i for i = 1,...,10 & obtaining level 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lars, HuberRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(desired_cols=['calculatedfinishedsquarefeet', 'bathroomcnt', 'structuretaxvaluedollarcnt', 'bedroomcnt', 'calculatedbathnbr', 'N-ValueRatio', 'N-ValueProp', 'N-LivingAreaProp'])), ('imputer', Imputer(...roomcnt'), ('regionidcounty', 'bedroomcnt'), ('regionidcounty', 'structuretaxvaluedollarcnt')]))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pipeline.fit(properties) #fitting the pipeline to the entire properties dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stage2_models = [\n",
    "    (\"ridge\",ElasticNet(alpha=1.25, l1_ratio = 0, max_iter=1000)),\n",
    "    (\"enet\", ElasticNet(alpha=1.25, l1_ratio = 0.5, max_iter=1000)),\n",
    "    (\"lasso\", ElasticNet(alpha=1.25, l1_ratio = 1, max_iter=1000)),\n",
    "    (\"larm\", Lars(n_nonzero_coefs = 1)),\n",
    "    (\"huber\", HuberRegressor())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training data into over/under subsets\n",
    "ix_overestimated = np.where(data['logerror'] >= 0)[0]\n",
    "ix_underestimated = np.where(data['logerror'] < 0)[0]\n",
    "data_indices = {\"over\": ix_overestimated, \"under\": ix_underestimated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: stacked_rfs\t \n",
      "...Current Model: ridge\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: enet\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: lasso\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: larm\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: huber\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Stage 1: stacked_annrfs\t \n",
      "...Current Model: ridge\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: enet\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: lasso\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: larm\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: huber\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Stage 1: logistic\t \n",
      "...Current Model: ridge\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: enet\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: lasso\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: larm\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "...Current Model: huber\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "level_one_data = data[['parcelid']].copy()\n",
    "\n",
    "for stage1_pair in stage1_models:\n",
    "    stage1_name, stage1_probs = stage1_pair\n",
    "    print(\"Stage 1: %s\\t \" % (stage1_name))\n",
    "    \n",
    "    for pair in stage2_models:\n",
    "        current_model_name,current_model = pair\n",
    "        print(\"...Current Model: %s\" % (current_model_name))\n",
    "\n",
    "        # initialize an NoneObject to be a placeholder for level-one data for current model\n",
    "        model_preds = None \n",
    "\n",
    "        for fold_nbr in range(1,11):\n",
    "            print(\"...working on fold %d\" % fold_nbr)\n",
    "\n",
    "            # set training data X \\ fold\n",
    "            fold_trainindices = np.setdiff1d(indices,fold_indices[fold_nbr])\n",
    "            fold_traindata = data.iloc[fold_trainindices,]\n",
    "\n",
    "            # training the over/under models on their respective training data\n",
    "            fold_preds_dict = {'over': None, 'under':None}\n",
    "            for key,val in data_indices.items():\n",
    "                type_of_zestimate, ix = key, val\n",
    "\n",
    "                # preprocess current training data\n",
    "                current_traindata = data.iloc[np.intersect1d(ix, fold_trainindices),]\n",
    "\n",
    "                # get a clone of the model and fit the current training data\n",
    "                print('......training model')\n",
    "                reg = clone(current_model)\n",
    "                reg.fit(feature_pipeline.transform(current_traindata), current_traindata['logerror'])\n",
    "\n",
    "                # level-one data (i.e., predict observations on current fold using reg)\n",
    "                print('......obtaining level-one data')\n",
    "                fold_data = data.iloc[fold_indices[fold_nbr]]\n",
    "                fold_preds_overunder = Series(reg.predict(feature_pipeline.transform(fold_data)), \n",
    "                                    index=fold_indices[fold_nbr], name = current_model_name)\n",
    "                fold_preds_dict[type_of_zestimate] = fold_preds_overunder\n",
    "\n",
    "            # combine over/under fold preds to get a single prediction\n",
    "            fold_stage1_overestimate_probs = stage1_probs.iloc[fold_indices[fold_nbr]]['overestimate_prob'] \n",
    "            fold_preds = (fold_preds_dict['over']*fold_stage1_overestimate_probs\n",
    "                              + fold_preds_dict['under']*(1-fold_stage1_overestimate_probs))\n",
    "            fold_preds.name = stage1_name + '_' + current_model_name\n",
    "\n",
    "            # adding to the placeholder for level-one data\n",
    "            if model_preds is not None:\n",
    "                model_preds = pd.concat([model_preds, fold_preds])\n",
    "            else:\n",
    "                model_preds = fold_preds\n",
    "\n",
    "            # some housecleaning\n",
    "            del reg\n",
    "\n",
    "        # add level-one predictions of current model to running dataframe\n",
    "        level_one_data = pd.concat([level_one_data, model_preds], axis=1)\n",
    "        print(\"\")\n",
    "    \n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>stacked_rfs_ridge</th>\n",
       "      <th>stacked_rfs_enet</th>\n",
       "      <th>stacked_rfs_lasso</th>\n",
       "      <th>stacked_rfs_larm</th>\n",
       "      <th>stacked_rfs_huber</th>\n",
       "      <th>stacked_annrfs_ridge</th>\n",
       "      <th>stacked_annrfs_enet</th>\n",
       "      <th>stacked_annrfs_lasso</th>\n",
       "      <th>stacked_annrfs_larm</th>\n",
       "      <th>stacked_annrfs_huber</th>\n",
       "      <th>logistic_ridge</th>\n",
       "      <th>logistic_enet</th>\n",
       "      <th>logistic_lasso</th>\n",
       "      <th>logistic_larm</th>\n",
       "      <th>logistic_huber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17073783</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.025774</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.021019</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>0.019120</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>0.010649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17088994</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>0.002048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17100444</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.002524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17102429</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.015419</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.010865</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>0.008784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17109604</td>\n",
       "      <td>0.018363</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.015377</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.020961</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>0.017030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  stacked_rfs_ridge  stacked_rfs_enet  stacked_rfs_lasso  \\\n",
       "0  17073783           0.024176          0.025774           0.026020   \n",
       "1  17088994           0.004663          0.003873           0.003904   \n",
       "2  17100444           0.012671          0.011883           0.011940   \n",
       "3  17102429           0.012099          0.010563           0.010574   \n",
       "4  17109604           0.018363          0.017627           0.017683   \n",
       "\n",
       "   stacked_rfs_larm  stacked_rfs_huber  stacked_annrfs_ridge  \\\n",
       "0          0.027547           0.014576              0.018770   \n",
       "1          0.004764           0.000855              0.006679   \n",
       "2          0.012623           0.007172              0.016844   \n",
       "3          0.010785           0.008539              0.020774   \n",
       "4          0.015224           0.014375              0.016069   \n",
       "\n",
       "   stacked_annrfs_enet  stacked_annrfs_lasso  stacked_annrfs_larm  \\\n",
       "0             0.019923              0.020110             0.021019   \n",
       "1             0.005929              0.006016             0.007209   \n",
       "2             0.016133              0.016214             0.017172   \n",
       "3             0.018797              0.018773             0.019321   \n",
       "4             0.015316              0.015377             0.012670   \n",
       "\n",
       "   stacked_annrfs_huber  logistic_ridge  logistic_enet  logistic_lasso  \\\n",
       "0              0.011123        0.018028       0.019120        0.019298   \n",
       "1              0.001368        0.009354       0.008658        0.008818   \n",
       "2              0.009779        0.005232       0.004307        0.004321   \n",
       "3              0.015419        0.012408       0.010856        0.010865   \n",
       "4              0.012031        0.020961       0.020245        0.020294   \n",
       "\n",
       "   logistic_larm  logistic_huber  \n",
       "0       0.020122        0.010649  \n",
       "1       0.010454        0.002048  \n",
       "2       0.004514        0.002524  \n",
       "3       0.011089        0.008784  \n",
       "4       0.018116        0.017030  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_one_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'levelonedata/l1data_twostage_linear_models.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-5565243d78ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlevel_one_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"levelonedata/l1data_twostage_linear_models.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/anerdi/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1401\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1403\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anerdi/anaconda3/lib/python3.5/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m   1576\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anerdi/anaconda3/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'levelonedata/l1data_twostage_linear_models.csv'"
     ]
    }
   ],
   "source": [
    "level_one_data.to_csv(\"~/home/an/levelonedata/l1data_twostage_linear_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
