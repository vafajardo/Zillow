{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_submissions(oct_model,nov_model,dec_model,name='new_submission',logy=True):\n",
    "    \"\"\"\n",
    "    This function creates the submission file for the public leaderboard predictions.\n",
    "    Three already fitted models, one for each of the predicting time points, is required.\n",
    "    \"\"\"\n",
    "    submission_df = DataFrame()\n",
    "    for i in range(int(properties.shape[0] / 100000)):\n",
    "        all_feats = full_pipeline.transform(properties.iloc[i*100000:(i+1)*100000])\n",
    "        foo = properties.iloc[i*100000:(i+1)*100000][['parcelid']].reset_index(drop=True)\n",
    "        if logy:\n",
    "            foo = pd.concat([foo, DataFrame({'201610': oct_model.predict(all_feats),\n",
    "                                                            '201611': nov_model.predict(all_feats),\n",
    "                                                            '201612': dec_model.predict(all_feats)})], axis=1)\n",
    "        else:\n",
    "            foo = pd.concat([foo, DataFrame({'201610': np.log(oct_model.predict(all_feats)),\n",
    "                                                            '201611': np.log(nov_model.predict(all_feats)),\n",
    "                                                            '201612': np.log(dec_model.predict(all_feats))})], axis=1)\n",
    "        submission_df = pd.concat([submission_df, foo], ignore_index=True)\n",
    "\n",
    "    #  fencepost problem\n",
    "    all_feats = full_pipeline.transform(properties.iloc[2900000:])\n",
    "    foo = properties.iloc[2900000:][['parcelid']].reset_index(drop=True)\n",
    "    foo = pd.concat([foo, DataFrame({'201610': oct_model.predict(all_feats),\n",
    "                                                    '201611': nov_model.predict(all_feats),\n",
    "                                                    '201612': dec_model.predict(all_feats)})], axis=1)\n",
    "    submission_df = pd.concat([submission_df, foo], ignore_index=True)\n",
    "    \n",
    "    submission_df['201710'] = 0\n",
    "    submission_df['201711'] = 0\n",
    "    submission_df['201712'] = 0\n",
    "    \n",
    "    submission_df.rename(columns={'parcelid':'ParcelId'}, inplace=True)    \n",
    "#     submission_df[['201610','201611','201612','201710','201711','201712']]= submission_df[['201610','201611','201612',\n",
    "#                                                                                            '201710','201711','201712']].round(4)\n",
    "    # unit test\n",
    "    submission_df.drop_duplicates(inplace=True)\n",
    "    assert submission_df.shape[0] == properties.shape[0]\n",
    "    # write to .csv\n",
    "    submission_df[['ParcelId','201610','201611','201612',\n",
    "                  '201710','201711','201712']].to_csv(name + \".gz\", index=False, float_format='%.4g', compression='gzip')\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_errors(submission_df, comparison_df):\n",
    "    \"\"\"\n",
    "    This function takes a submission entry for public leaderboard, and returns\n",
    "    the training error for each month.\n",
    "    \"\"\"\n",
    "    # training error\n",
    "    trainresults = pd.merge(submission_df[['ParcelId','201610','201611','201612']], comparison_df[['parcelid','logerror','month']],\n",
    "                           left_on='ParcelId', right_on='parcelid')\n",
    "    oct_error = abs(trainresults[trainresults['month'] == 10]['201610'] \n",
    "                    - trainresults[trainresults['month'] == 10]['logerror']).mean()\n",
    "    nov_error = abs(trainresults[trainresults['month'] == 11]['201611'] \n",
    "                    - trainresults[trainresults['month'] == 11]['logerror']).mean()\n",
    "    dec_error = abs(trainresults[trainresults['month'] == 12]['201612'] \n",
    "                    - trainresults[trainresults['month'] == 12]['logerror']).mean()\n",
    "    overall_mae = (oct_error*(trainresults['month'] == 10).sum() + nov_error*(trainresults['month'] == 11).sum() \n",
    "                        + dec_error*(trainresults['month'] == 12).sum()) / (trainresults['month'].isin([10,11,12])).sum()\n",
    "    return (oct_error, nov_error, dec_error, overall_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anerdi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\")\n",
    "test_parcels = pd.read_csv(maindir + \"/data/sample_submission.csv\", usecols = ['ParcelId'])\n",
    "\n",
    "test_parcels.rename(columns={'ParcelId':'parcelid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#life of property\n",
    "properties['N-life'] = 2018 - properties['yearbuilt']\n",
    "\n",
    "#error in calculation of the finished living area of home\n",
    "properties['N-LivingAreaError'] = properties['calculatedfinishedsquarefeet']/properties['finishedsquarefeet12']\n",
    "\n",
    "#proportion of living area\n",
    "properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet']/properties['lotsizesquarefeet']\n",
    "properties['N-LivingAreaProp2'] = properties['finishedsquarefeet12']/properties['finishedsquarefeet15']\n",
    "\n",
    "#Amout of extra space\n",
    "properties['N-ExtraSpace'] = properties['lotsizesquarefeet'] - properties['calculatedfinishedsquarefeet'] \n",
    "properties['N-ExtraSpace-2'] = properties['finishedsquarefeet15'] - properties['finishedsquarefeet12'] \n",
    "\n",
    "#Total number of rooms\n",
    "properties['N-TotalRooms'] = properties['bathroomcnt']*properties['bedroomcnt']\n",
    "\n",
    "#Average room size\n",
    "properties['N-AvRoomSize'] = properties['calculatedfinishedsquarefeet']/properties['roomcnt'] \n",
    "\n",
    "# Number of Extra rooms\n",
    "properties['N-ExtraRooms'] = properties['roomcnt'] - properties['N-TotalRooms'] \n",
    "\n",
    "#Ratio of the built structure value to land area\n",
    "properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt']/properties['landtaxvaluedollarcnt']\n",
    "\n",
    "#Does property have a garage, pool or hot tub and AC?\n",
    "properties['N-GarPoolAC'] = ((properties['garagecarcnt']>0) & (properties['pooltypeid10']>0) & (properties['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "properties[\"N-location\"] = properties[\"latitude\"] + properties[\"longitude\"]\n",
    "properties[\"N-location-2\"] = properties[\"latitude\"]*properties[\"longitude\"]\n",
    "properties[\"N-location-2round\"] = properties[\"N-location-2\"].round(-4)\n",
    "\n",
    "properties[\"N-latitude-round\"] = properties[\"latitude\"].round(-4)\n",
    "properties[\"N-longitude-round\"] = properties[\"longitude\"].round(-4)\n",
    "\n",
    "#Ratio of tax of property over parcel\n",
    "properties['N-ValueRatio'] = properties['taxvaluedollarcnt']/properties['taxamount']\n",
    "\n",
    "#TotalTaxScore\n",
    "properties['N-TaxScore'] = properties['taxvaluedollarcnt']*properties['taxamount']\n",
    "\n",
    "#polnomials of tax delinquency year\n",
    "properties[\"N-taxdelinquencyyear-2\"] = properties[\"taxdelinquencyyear\"] ** 2\n",
    "properties[\"N-taxdelinquencyyear-3\"] = properties[\"taxdelinquencyyear\"] ** 3\n",
    "\n",
    "#Length of time since unpaid taxes\n",
    "properties['N-life'] = 2018 - properties['taxdelinquencyyear']\n",
    "\n",
    "#Number of properties in the zip\n",
    "zip_count = properties['regionidzip'].value_counts().to_dict()\n",
    "properties['N-zip_count'] = properties['regionidzip'].map(zip_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "city_count = properties['regionidcity'].value_counts().to_dict()\n",
    "properties['N-city_count'] = properties['regionidcity'].map(city_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "region_count = properties['regionidcounty'].value_counts().to_dict()\n",
    "properties['N-county_count'] = properties['regionidcounty'].map(region_count)\n",
    "\n",
    "#Average structuretaxvaluedollarcnt by city\n",
    "group = properties.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "properties['N-Avg-structuretaxvaluedollarcnt'] = properties['regionidcity'].map(group)\n",
    "\n",
    "#Deviation away from average\n",
    "properties['N-Dev-structuretaxvaluedollarcnt'] = (abs((properties['structuretaxvaluedollarcnt'] \n",
    "                                                       - properties['N-Avg-structuretaxvaluedollarcnt']))\n",
    "                                                  /properties['N-Avg-structuretaxvaluedollarcnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in predictions from the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06335799788964462,\n",
       " 0.06250493564507772,\n",
       " 0.07474658919966297,\n",
       " 0.065494156708368903)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LME = pd.read_csv(\"test_predictions.csv.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(LME, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0627264215390796,\n",
       " 0.06182163198247547,\n",
       " 0.07473668775158138,\n",
       " 0.064978084757667934)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = pd.read_csv(\"RF_n100_maxfeat5_maxdepth8.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(RF, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06195625879043588,\n",
       " 0.06436095290251921,\n",
       " 0.07321730879815982,\n",
       " 0.064762854132521586)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_2 = pd.read_csv(\"RF_n100_maxfeat10_maxdepth20_extreme.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(RF_2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06360390734619255,\n",
       " 0.06268171032858727,\n",
       " 0.07535184221736627,\n",
       " 0.065798443401779524)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge = pd.read_csv(\"Ridge.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(Ridge, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06356341482419092,\n",
       " 0.06265443740963851,\n",
       " 0.07534662269695212,\n",
       " 0.065767957756965326)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enet = pd.read_csv(\"ElasticNet.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(Enet, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06354550414788025,\n",
       " 0.06262862377984654,\n",
       " 0.07529440752731466,\n",
       " 0.065741373900257555)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso = pd.read_csv(\"Lasso.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(Lasso, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06211727600763514,\n",
       " 0.06136520946002188,\n",
       " 0.07414289916043706,\n",
       " 0.064404712807773379)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB = pd.read_csv(\"XGB_600.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(XGB, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05998706659714675,\n",
       " 0.05918412810186208,\n",
       " 0.07170160296434737,\n",
       " 0.06220029687696086)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB3000 = pd.read_csv(\"XGB_3000.csv\")\n",
    "mean_absolute_errors(XGB3000, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06361133815551537,\n",
       " 0.06264460021905807,\n",
       " 0.0749035882691202,\n",
       " 0.065703583469913368)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Huber = pd.read_csv(\"HuberRegressor.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(Huber, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06361498894916652,\n",
       " 0.06264090909090905,\n",
       " 0.07490658999424941,\n",
       " 0.065705532662140173)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LARM_LASSO =  pd.read_csv(\"LARM_LASSO.gz\", compression=\"gzip\")\n",
    "mean_absolute_errors(LARM_LASSO, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [(Ridge,0.13),\n",
    "         (Enet,0.13),\n",
    "         (Lasso,0.13),\n",
    "         (LARM_LASSO,0.14),\n",
    "         (LME, 0.14), \n",
    "         (XGB,0.13),\n",
    "         (RF,0.13),\n",
    "         (XGB3000, 0.035),\n",
    "         (RF_2, 0.035)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([y for x,y in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = Ridge[['ParcelId']].copy()\n",
    "cols = ['201610','201611','201612','201710','201711','201712']\n",
    "foo = models[0][0][cols]*models[0][1]\n",
    "for pair in models[1:]:\n",
    "    model,wt = pair\n",
    "    foo = foo + model[cols]*wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06253640058108395,\n",
       " 0.06168207707014895,\n",
       " 0.074036725003617,\n",
       " 0.06469503666628855)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = pd.concat([ensemble,foo], axis=1)\n",
    "ensemble['ParcelId'] = ensemble['ParcelId'].astype(int)\n",
    "assert all(ensemble.ParcelId.unique() == Ridge.ParcelId.unique())\n",
    "mean_absolute_errors(ensemble, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble.to_csv(\"new_ensemble.gz\", index=False, float_format='%.4g', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
