{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import feature_pipelines as pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anerdi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proportion of living area\n",
    "properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet']/properties['lotsizesquarefeet']\n",
    "\n",
    "#Ratio of the built structure value to land area\n",
    "properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt']/properties['landtaxvaluedollarcnt']\n",
    "\n",
    "#Ratio of tax of property over parcel\n",
    "properties['N-ValueRatio'] = properties['taxvaluedollarcnt']/properties['taxamount']\n",
    "\n",
    "# Pool\n",
    "properties['Pool'] = (properties['pooltypeid2'].fillna(0) + properties['pooltypeid7'].fillna(0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')\n",
    "del logerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New response variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['overestimation'] = (data['logerror'] >= 0).astype(int)\n",
    "data['extreme_overestimation'] = (data['logerror'] >= 1.5).astype(int)\n",
    "data['extreme_underestimation'] = (data['logerror'] <= -1.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup variables considered in the model\n",
    "\n",
    "\n",
    "# numerical variables\n",
    "num_atts = ['bathroomcnt','bedroomcnt','buildingqualitytypeid','calculatedbathnbr','finishedfloor1squarefeet',\n",
    "           'calculatedfinishedsquarefeet','finishedsquarefeet12','finishedsquarefeet13',\n",
    "           'finishedsquarefeet15','finishedsquarefeet50','finishedsquarefeet6','fireplacecnt',\n",
    "           'fullbathcnt','garagecarcnt','garagetotalsqft','latitude','longitude','lotsizesquarefeet',\n",
    "           'poolcnt','poolsizesum','censustractandblock','roomcnt','threequarterbathnbr','unitcnt',\n",
    "           'yardbuildingsqft17','yardbuildingsqft26','numberofstories',\n",
    "            'structuretaxvaluedollarcnt','taxvaluedollarcnt','landtaxvaluedollarcnt','taxamount',\n",
    "           'N-ValueRatio', 'N-LivingAreaProp', 'N-ValueProp']\n",
    "\n",
    "# categorical varaibles\n",
    "cat_atts = ['airconditioningtypeid','architecturalstyletypeid',\n",
    "           'buildingclasstypeid','heatingorsystemtypeid','pooltypeid10','pooltypeid2',\n",
    "            'pooltypeid7','propertylandusetypeid','regionidcounty',\n",
    "           'storytypeid','typeconstructiontypeid','yearbuilt','fireplaceflag',\n",
    "           'taxdelinquencyflag']\n",
    "\n",
    "# Dictionary of categorical variables and their default levels\n",
    "cat_dict = {key:value for key,value in {'airconditioningtypeid':[-1] + list(range(1,14)),\n",
    "           'architecturalstyletypeid':[-1] + list(range(1,28)),\n",
    "           'buildingclasstypeid':[-1] + list(range(1,6)),\n",
    "            'heatingorsystemtypeid':[-1] + list(range(1,26)),\n",
    "            'pooltypeid10': list(range(-1,2)),\n",
    "            'pooltypeid2': list(range(-1,2)),\n",
    "            'pooltypeid7': list(range(-1,2)),\n",
    "            'Pool': [0,1],\n",
    "            'propertylandusetypeid': [-1, 31,46,47,246,247,248,260,261,262,263,264,265,266,267,268,269,270,271,\n",
    "                                     273,274,275,276,279,290,291],\n",
    "            'regionidcounty': [2061,3101,1286],\n",
    "            'storytypeid':[-1] + list(range(1,36)),\n",
    "            'typeconstructiontypeid':[-1] + list(range(1,19)),\n",
    "            'yearbuilt': [-1] + list(range(1885,2018)),\n",
    "            'fireplaceflag': [-1] + ['True','False'],\n",
    "            'taxdelinquencyflag': [-1] + ['Y','N']\n",
    "           }.items() if key in cat_atts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "        ('select_and_dummify', pipes.DF_Selector_GetDummies(cat_dict)),\n",
    "    ])\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', pipes.DataFrameSelector(num_atts)),\n",
    "        ('imputer', Imputer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# Full pipeline\n",
    "feature_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into the 10-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2199, 86155, 84691, ..., 86952, 82677, 76398])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(19)\n",
    "np.random.shuffle(indices) # in-place shuffling \n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_indices = {(i+1):indices[i::10] for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 2199, 83721, 29492, ..., 37852, 48911, 39220]),\n",
       " 2: array([86155, 32252, 81949, ..., 57319, 13479, 33811]),\n",
       " 3: array([84691, 37597,  3215, ..., 84821, 43372, 86952]),\n",
       " 4: array([11172, 67082, 58364, ..., 74500, 63830, 82677]),\n",
       " 5: array([78769, 73075, 17232, ..., 12489,   266, 76398]),\n",
       " 6: array([53035, 17238, 32604, ..., 14649, 26827, 61025]),\n",
       " 7: array([58194, 72307,  3380, ..., 57397, 68361, 53125]),\n",
       " 8: array([66378, 81551, 66156, ..., 73922, 85799, 45218]),\n",
       " 9: array([70318, 70507, 20646, ...,  7537, 69584, 17218]),\n",
       " 10: array([42552, 66817, 57336, ..., 88913, 67815, 17738])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1 Classification - P(overestimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models on the 10 splits of data \\ fold_i for i = 1,...,10 & obtaining level 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(desired_cols=['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'fi... 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"network\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 50), random_state=1)),\n",
    "    (\"logistic\", LogisticRegression(penalty='l1', tol=0.01)),\n",
    "    (\"rf_1\",RandomForestClassifier(max_depth=8, random_state=9,class_weight={0:0.52,1:0.48})),\n",
    "    (\"rf_2\", RandomForestClassifier(max_depth=12, random_state=9)),\n",
    "    (\"rf_3\", RandomForestClassifier(max_depth=12, random_state=9, class_weight={0:0.55,1:0.45})),\n",
    "    (\"xgb_1\", XGBClassifier(max_depth=7, random_state=9, n_jobs=2)),\n",
    "    (\"xgb_2\", XGBClassifier(max_depth=5, random_state=9, n_jobs=2)),\n",
    "    (\"lgbm_1\", LGBMClassifier(random_state=9, n_jobs=2, n_estimators=100)),\n",
    "    (\"lgbm_2\", LGBMClassifier(random_state=9, n_jobs=2, n_estimators=100, boosting_type = 'goss'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: network\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: logistic\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: rf_1\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: rf_2\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: rf_3\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: xgb_1\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: xgb_2\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: lgbm_1\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "Current model: lgbm_2\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 4\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 5\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 6\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 7\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 8\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 9\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 10\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "level_one_data = data[['parcelid']].copy()\n",
    "\n",
    "for pair in models:\n",
    "    current_model_name,current_model = pair\n",
    "    print(\"Current model: %s\" % current_model_name)\n",
    "    \n",
    "    # initialize an NoneObject to be a placeholder for level-one data for current model\n",
    "    model_preds = None \n",
    "    \n",
    "    for fold_nbr in range(1,11):\n",
    "        print(\"...working on fold %d\" % fold_nbr)\n",
    "\n",
    "        # set training data X \\ fold\n",
    "        current_traindata = data.iloc[np.setdiff1d(indices,fold_indices[fold_nbr]),]\n",
    "\n",
    "        # get a clone of the model and fit the current training data\n",
    "        print('......training model')\n",
    "        clf = clone(current_model)\n",
    "        clf.fit(feature_pipeline.transform(current_traindata), current_traindata['overestimation'])\n",
    "\n",
    "        # level-one data (i.e., predict observations on current fold using reg)\n",
    "        print('......obtaining level-one data')\n",
    "        fold_data = data.iloc[fold_indices[fold_nbr]]\n",
    "        fold_preds = Series(clf.predict_proba(feature_pipeline.transform(fold_data))[:,1], \n",
    "                            index=fold_indices[fold_nbr], name = current_model_name)\n",
    "\n",
    "        # adding to the placeholder for level-one data\n",
    "        if model_preds is not None:\n",
    "            model_preds = pd.concat([model_preds, fold_preds])\n",
    "        else:\n",
    "            model_preds = fold_preds\n",
    "\n",
    "        # some housecleaning\n",
    "        del clf\n",
    "    \n",
    "    # add level-one predictions of current model to running dataframe\n",
    "    level_one_data = pd.concat([level_one_data, model_preds], axis=1)\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>network</th>\n",
       "      <th>logistic</th>\n",
       "      <th>rf_1</th>\n",
       "      <th>rf_2</th>\n",
       "      <th>rf_3</th>\n",
       "      <th>xgb_1</th>\n",
       "      <th>xgb_2</th>\n",
       "      <th>lgbm_1</th>\n",
       "      <th>lgbm_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17073783</td>\n",
       "      <td>0.540814</td>\n",
       "      <td>0.586041</td>\n",
       "      <td>0.606695</td>\n",
       "      <td>0.725421</td>\n",
       "      <td>0.570743</td>\n",
       "      <td>0.703276</td>\n",
       "      <td>0.626319</td>\n",
       "      <td>0.600729</td>\n",
       "      <td>0.817952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17088994</td>\n",
       "      <td>0.575728</td>\n",
       "      <td>0.602346</td>\n",
       "      <td>0.520296</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.501637</td>\n",
       "      <td>0.485501</td>\n",
       "      <td>0.552616</td>\n",
       "      <td>0.529095</td>\n",
       "      <td>0.461712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17100444</td>\n",
       "      <td>0.618680</td>\n",
       "      <td>0.516813</td>\n",
       "      <td>0.527499</td>\n",
       "      <td>0.537820</td>\n",
       "      <td>0.490139</td>\n",
       "      <td>0.591807</td>\n",
       "      <td>0.581336</td>\n",
       "      <td>0.555954</td>\n",
       "      <td>0.585355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17102429</td>\n",
       "      <td>0.667181</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.535683</td>\n",
       "      <td>0.560426</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0.647888</td>\n",
       "      <td>0.661331</td>\n",
       "      <td>0.666883</td>\n",
       "      <td>0.656928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17109604</td>\n",
       "      <td>0.561772</td>\n",
       "      <td>0.588186</td>\n",
       "      <td>0.619031</td>\n",
       "      <td>0.568314</td>\n",
       "      <td>0.526031</td>\n",
       "      <td>0.475878</td>\n",
       "      <td>0.480612</td>\n",
       "      <td>0.461771</td>\n",
       "      <td>0.437258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid   network  logistic      rf_1      rf_2      rf_3     xgb_1  \\\n",
       "0  17073783  0.540814  0.586041  0.606695  0.725421  0.570743  0.703276   \n",
       "1  17088994  0.575728  0.602346  0.520296  0.538926  0.501637  0.485501   \n",
       "2  17100444  0.618680  0.516813  0.527499  0.537820  0.490139  0.591807   \n",
       "3  17102429  0.667181  0.594042  0.535683  0.560426  0.524229  0.647888   \n",
       "4  17109604  0.561772  0.588186  0.619031  0.568314  0.526031  0.475878   \n",
       "\n",
       "      xgb_2    lgbm_1    lgbm_2  \n",
       "0  0.626319  0.600729  0.817952  \n",
       "1  0.552616  0.529095  0.461712  \n",
       "2  0.581336  0.555954  0.585355  \n",
       "3  0.661331  0.666883  0.656928  \n",
       "4  0.480612  0.461771  0.437258  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_one_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90275"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_one_data.shape[0]==data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level_one_data.to_csv(\"/home/anerdi/Desktop/Zillow/levelonedata/stage1_l1data.csv.gz\", compression ='gzip',\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>network</th>\n",
       "      <th>logistic</th>\n",
       "      <th>rf_1</th>\n",
       "      <th>rf_2</th>\n",
       "      <th>rf_3</th>\n",
       "      <th>xgb_1</th>\n",
       "      <th>xgb_2</th>\n",
       "      <th>lgbm_1</th>\n",
       "      <th>lgbm_2</th>\n",
       "      <th>overestimation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17073783</td>\n",
       "      <td>0.540814</td>\n",
       "      <td>0.586041</td>\n",
       "      <td>0.606695</td>\n",
       "      <td>0.725421</td>\n",
       "      <td>0.570743</td>\n",
       "      <td>0.703276</td>\n",
       "      <td>0.626319</td>\n",
       "      <td>0.600729</td>\n",
       "      <td>0.817952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17088994</td>\n",
       "      <td>0.575728</td>\n",
       "      <td>0.602346</td>\n",
       "      <td>0.520296</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.501637</td>\n",
       "      <td>0.485501</td>\n",
       "      <td>0.552616</td>\n",
       "      <td>0.529095</td>\n",
       "      <td>0.461712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17100444</td>\n",
       "      <td>0.618680</td>\n",
       "      <td>0.516813</td>\n",
       "      <td>0.527499</td>\n",
       "      <td>0.537820</td>\n",
       "      <td>0.490139</td>\n",
       "      <td>0.591807</td>\n",
       "      <td>0.581336</td>\n",
       "      <td>0.555954</td>\n",
       "      <td>0.585355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17102429</td>\n",
       "      <td>0.667181</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.535683</td>\n",
       "      <td>0.560426</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0.647888</td>\n",
       "      <td>0.661331</td>\n",
       "      <td>0.666883</td>\n",
       "      <td>0.656928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17109604</td>\n",
       "      <td>0.561772</td>\n",
       "      <td>0.588186</td>\n",
       "      <td>0.619031</td>\n",
       "      <td>0.568314</td>\n",
       "      <td>0.526031</td>\n",
       "      <td>0.475878</td>\n",
       "      <td>0.480612</td>\n",
       "      <td>0.461771</td>\n",
       "      <td>0.437258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid   network  logistic      rf_1      rf_2      rf_3     xgb_1  \\\n",
       "0  17073783  0.540814  0.586041  0.606695  0.725421  0.570743  0.703276   \n",
       "1  17088994  0.575728  0.602346  0.520296  0.538926  0.501637  0.485501   \n",
       "2  17100444  0.618680  0.516813  0.527499  0.537820  0.490139  0.591807   \n",
       "3  17102429  0.667181  0.594042  0.535683  0.560426  0.524229  0.647888   \n",
       "4  17109604  0.561772  0.588186  0.619031  0.568314  0.526031  0.475878   \n",
       "\n",
       "      xgb_2    lgbm_1    lgbm_2  overestimation  \n",
       "0  0.626319  0.600729  0.817952               1  \n",
       "1  0.552616  0.529095  0.461712               1  \n",
       "2  0.581336  0.555954  0.585355               1  \n",
       "3  0.661331  0.666883  0.656928               0  \n",
       "4  0.480612  0.461771  0.437258               1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.merge(level_one_data, data[['overestimation','parcelid']], on='parcelid')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked = LogisticRegression(penalty='l1', tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.01,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.fit(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']], training_data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84417336, -0.1487704 , -0.3090904 ,  1.26490142,  0.6983782 ,\n",
       "        0.94914558,  0.7787071 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.04366505])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58702928408099242"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.score(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']], training_data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = stacked.predict(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']])\n",
    "true = training_data['overestimation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856824589421\n",
      "0.590619988584\n",
      "0.69924297885\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(true,pred))\n",
    "print(precision_score(true,pred))\n",
    "print(f1_score(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9683, 30123],\n",
       "       [ 7262, 43459]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(true, pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_ann = clone(models[0][1])\n",
    "clf_rf2 = clone(models[3][1])\n",
    "clf_rf3 = clone(models[4][1])\n",
    "clf_xgb1 = clone(models[5][1])\n",
    "clf_xgb2 = clone(models[6][1])\n",
    "clf_lgbm1 = clone(models[7][1])\n",
    "clf_lgbm2 = clone(models[8][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='goss', colsample_bytree=1.0, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=10,\n",
       "        min_child_weight=5, min_split_gain=0.0, n_estimators=100, n_jobs=2,\n",
       "        num_leaves=31, objective=None, random_state=9, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ann.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_rf2.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_rf3.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_xgb1.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_xgb2.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_lgbm1.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_lgbm2.fit(feature_pipeline.transform(data), data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [('ann',clf_ann),\n",
    "          ('rf2',clf_rf2),\n",
    "          ('rf3',clf_rf3),\n",
    "          ('xgb1',clf_xgb1),\n",
    "          ('xgb2',clf_xgb2),\n",
    "          ('lgbm1',clf_lgbm1),\n",
    "          ('lgbm2',clf_lgbm2)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overestimate_probs = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])\n",
    "for pair in models:\n",
    "    model_name, model = pair\n",
    "    probabilities = None\n",
    "    for i in range(int(properties.shape[0] / 100000)):   \n",
    "        # get current test features\n",
    "        current_test_feats = feature_pipeline.transform(properties.iloc[i*100000:(i+1)*100000])\n",
    "\n",
    "        # predict on current test obs\n",
    "        current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='%s_overestimate_prob'%model_name,\n",
    "                              index = np.arange(i*100000,(i+1)*100000))\n",
    "\n",
    "        if probabilities is not None:\n",
    "            probabilities = pd.concat([probabilities, current_probs])\n",
    "        else:\n",
    "            probabilities = current_probs\n",
    "\n",
    "    #  fencepost problem\n",
    "    current_test_feats = feature_pipeline.transform(properties.iloc[2900000:])\n",
    "    current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='%s_overestimate_prob'%model_name,\n",
    "                          index = np.arange(2900000,2985217))\n",
    "    probabilities = pd.concat([probabilities, current_probs])\n",
    "    overestimate_probs = pd.concat([overestimate_probs, probabilities], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overestimate_probs['stacked_pred'] = 1 / (1 + np.exp(-stacked.intercept_[0]\n",
    "         - overestimate_probs['ann_overestimate_prob']*stacked.coef_[0][0]\n",
    "            - overestimate_probs['rf2_overestimate_prob']*stacked.coef_[0][1]\n",
    "                - overestimate_probs['rf3_overestimate_prob']*stacked.coef_[0][2]\n",
    "                     - overestimate_probs['xgb1_overestimate_prob']*stacked.coef_[0][3]\n",
    "                        - overestimate_probs['xgb2_overestimate_prob']*stacked.coef_[0][4]\n",
    "                            - overestimate_probs['lgbm1_overestimate_prob']*stacked.coef_[0][5]\n",
    "                                - overestimate_probs['lgbm2_overestimate_prob']*stacked.coef_[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>ann_overestimate_prob</th>\n",
       "      <th>rf2_overestimate_prob</th>\n",
       "      <th>rf3_overestimate_prob</th>\n",
       "      <th>xgb1_overestimate_prob</th>\n",
       "      <th>xgb2_overestimate_prob</th>\n",
       "      <th>lgbm1_overestimate_prob</th>\n",
       "      <th>lgbm2_overestimate_prob</th>\n",
       "      <th>stacked_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.469632</td>\n",
       "      <td>0.466012</td>\n",
       "      <td>0.495383</td>\n",
       "      <td>0.658496</td>\n",
       "      <td>0.665265</td>\n",
       "      <td>0.631455</td>\n",
       "      <td>0.529722</td>\n",
       "      <td>0.608192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.406392</td>\n",
       "      <td>0.555562</td>\n",
       "      <td>0.524675</td>\n",
       "      <td>0.536781</td>\n",
       "      <td>0.519523</td>\n",
       "      <td>0.552883</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.523073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.880731</td>\n",
       "      <td>0.548264</td>\n",
       "      <td>0.548752</td>\n",
       "      <td>0.430116</td>\n",
       "      <td>0.485831</td>\n",
       "      <td>0.575907</td>\n",
       "      <td>0.513572</td>\n",
       "      <td>0.569187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.569588</td>\n",
       "      <td>0.663067</td>\n",
       "      <td>0.543329</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.547760</td>\n",
       "      <td>0.614516</td>\n",
       "      <td>0.622471</td>\n",
       "      <td>0.617759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.540791</td>\n",
       "      <td>0.519636</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.502730</td>\n",
       "      <td>0.516258</td>\n",
       "      <td>0.486744</td>\n",
       "      <td>0.397359</td>\n",
       "      <td>0.488352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  ann_overestimate_prob  rf2_overestimate_prob  \\\n",
       "0  10754147               0.469632               0.466012   \n",
       "1  10759547               0.406392               0.555562   \n",
       "2  10843547               0.880731               0.548264   \n",
       "3  10859147               0.569588               0.663067   \n",
       "4  10879947               0.540791               0.519636   \n",
       "\n",
       "   rf3_overestimate_prob  xgb1_overestimate_prob  xgb2_overestimate_prob  \\\n",
       "0               0.495383                0.658496                0.665265   \n",
       "1               0.524675                0.536781                0.519523   \n",
       "2               0.548752                0.430116                0.485831   \n",
       "3               0.543329                0.679050                0.547760   \n",
       "4               0.485341                0.502730                0.516258   \n",
       "\n",
       "   lgbm1_overestimate_prob  lgbm2_overestimate_prob  stacked_pred  \n",
       "0                 0.631455                 0.529722      0.608192  \n",
       "1                 0.552883                 0.605128      0.523073  \n",
       "2                 0.575907                 0.513572      0.569187  \n",
       "3                 0.614516                 0.622471      0.617759  \n",
       "4                 0.486744                 0.397359      0.488352  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overestimate_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overestimate_probs.to_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs_stacked_ann_rfs_xgbs_lgbms.csv.gz\", \n",
    "                          index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Layer 2 P(extreme_overestimation | overestimation) and P(extreme_underestimation | underestimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_over = feature_pipeline.fit_transform(data[data['overestimation'] == 1])\n",
    "y_extreme_over = data[data['overestimation'] == 1]['extreme_overestimation']\n",
    "X_under = feature_pipeline.fit_transform(data[data['overestimation'] == 0])\n",
    "y_extreme_under = data[data['overestimation'] == 0]['extreme_underestimation']\n",
    "\n",
    "assert X_over.shape[0] == y_extreme_over.shape[0]\n",
    "assert X_under.shape[0] == y_extreme_under.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=15, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=9,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_layer2_over = RandomForestClassifier(max_depth=15, random_state=9, class_weight='balanced')\n",
    "clf_rf_layer2_over.fit(X_over, y_extreme_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=12, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=9,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_layer2_under = RandomForestClassifier(max_depth=12, random_state=9, class_weight='balanced')\n",
    "clf_rf_layer2_under.fit(X_under, y_extreme_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50436,    33],\n",
       "       [   70,    69]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(clf_rf_layer2_over.predict(X_over), y_extreme_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39520,    17],\n",
       "       [   88,    42]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(clf_rf_layer2_under.predict(X_under), y_extreme_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [('over',clf_rf_layer2_over),('under',clf_rf_layer2_under)]\n",
    "extreme_probs = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])\n",
    "for pair in models:\n",
    "    model_name, model = pair\n",
    "    probabilities = None\n",
    "    for i in range(int(properties.shape[0] / 100000)):   \n",
    "        # get current test features\n",
    "        current_test_feats = feature_pipeline.transform(properties.iloc[i*100000:(i+1)*100000])\n",
    "\n",
    "        # predict on current test obs\n",
    "        current_probs = DataFrame(model.predict_proba(current_test_feats), name='extreme_%s_prob'%model_name,\n",
    "                              index = np.arange(i*100000,(i+1)*100000))\n",
    "\n",
    "        if probabilities is not None:\n",
    "            probabilities = pd.concat([probabilities, current_probs])\n",
    "        else:\n",
    "            probabilities = current_probs\n",
    "\n",
    "    #  fencepost problem\n",
    "    current_test_feats = feature_pipeline.transform(properties.iloc[2900000:])\n",
    "    current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='extreme_%s_prob'%model_name,\n",
    "                          index = np.arange(2900000,2985217))\n",
    "    probabilities = pd.concat([probabilities, current_probs])\n",
    "    extreme_probs = pd.concat([extreme_probs, probabilities], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>extreme_over_prob</th>\n",
       "      <th>extreme_under_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.544759</td>\n",
       "      <td>0.192589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.283555</td>\n",
       "      <td>0.220792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.124701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.184405</td>\n",
       "      <td>0.136827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.238265</td>\n",
       "      <td>0.059137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  extreme_over_prob  extreme_under_prob\n",
       "0  10754147           0.544759            0.192589\n",
       "1  10759547           0.283555            0.220792\n",
       "2  10843547           0.357077            0.124701\n",
       "3  10859147           0.184405            0.136827\n",
       "4  10879947           0.238265            0.059137"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (overestimate_probs['parcelid'] == extreme_probs['parcelid']).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs = pd.merge(overestimate_probs[['parcelid','stacked_pred']], extreme_probs, on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs['over_but_not_extreme'] = overall_probs['stacked_pred']*(1 - overall_probs['extreme_over_prob'])\n",
    "overall_probs['over_and_extreme'] = overall_probs['stacked_pred']*overall_probs['extreme_over_prob']\n",
    "overall_probs['under_but_not_extreme'] = (1 - overall_probs['stacked_pred'])*(1 - overall_probs['extreme_under_prob'])\n",
    "overall_probs['under_and_extreme'] = (1 - overall_probs['stacked_pred'])*overall_probs['extreme_under_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>stacked_pred</th>\n",
       "      <th>extreme_over_prob</th>\n",
       "      <th>extreme_under_prob</th>\n",
       "      <th>over_but_not_extreme</th>\n",
       "      <th>over_and_extreme</th>\n",
       "      <th>under_but_not_extreme</th>\n",
       "      <th>under_and_extreme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.473844</td>\n",
       "      <td>0.544759</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.215713</td>\n",
       "      <td>0.258131</td>\n",
       "      <td>0.424824</td>\n",
       "      <td>0.101332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.474768</td>\n",
       "      <td>0.283555</td>\n",
       "      <td>0.220792</td>\n",
       "      <td>0.340146</td>\n",
       "      <td>0.134623</td>\n",
       "      <td>0.409265</td>\n",
       "      <td>0.115967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.732197</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.124701</td>\n",
       "      <td>0.470746</td>\n",
       "      <td>0.261451</td>\n",
       "      <td>0.234407</td>\n",
       "      <td>0.033395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.606960</td>\n",
       "      <td>0.184405</td>\n",
       "      <td>0.136827</td>\n",
       "      <td>0.495034</td>\n",
       "      <td>0.111926</td>\n",
       "      <td>0.339262</td>\n",
       "      <td>0.053778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.529348</td>\n",
       "      <td>0.238265</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>0.403223</td>\n",
       "      <td>0.126125</td>\n",
       "      <td>0.442819</td>\n",
       "      <td>0.027833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  stacked_pred  extreme_over_prob  extreme_under_prob  \\\n",
       "0  10754147      0.473844           0.544759            0.192589   \n",
       "1  10759547      0.474768           0.283555            0.220792   \n",
       "2  10843547      0.732197           0.357077            0.124701   \n",
       "3  10859147      0.606960           0.184405            0.136827   \n",
       "4  10879947      0.529348           0.238265            0.059137   \n",
       "\n",
       "   over_but_not_extreme  over_and_extreme  under_but_not_extreme  \\\n",
       "0              0.215713          0.258131               0.424824   \n",
       "1              0.340146          0.134623               0.409265   \n",
       "2              0.470746          0.261451               0.234407   \n",
       "3              0.495034          0.111926               0.339262   \n",
       "4              0.403223          0.126125               0.442819   \n",
       "\n",
       "   under_and_extreme  \n",
       "0           0.101332  \n",
       "1           0.115967  \n",
       "2           0.033395  \n",
       "3           0.053778  \n",
       "4           0.027833  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2985217.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_probs[['over_but_not_extreme','over_and_extreme','under_but_not_extreme','under_and_extreme']].sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs.to_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/two_layer_probabilities.csv.gz\", \n",
    "                          index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
