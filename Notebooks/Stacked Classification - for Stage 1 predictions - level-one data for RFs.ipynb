{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import feature_pipelines as pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anerdi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proportion of living area\n",
    "properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet']/properties['lotsizesquarefeet']\n",
    "\n",
    "#Ratio of the built structure value to land area\n",
    "properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt']/properties['landtaxvaluedollarcnt']\n",
    "\n",
    "#Ratio of tax of property over parcel\n",
    "properties['N-ValueRatio'] = properties['taxvaluedollarcnt']/properties['taxamount']\n",
    "\n",
    "# Pool\n",
    "properties['Pool'] = (properties['pooltypeid2'].fillna(0) + properties['pooltypeid7'].fillna(0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')\n",
    "del logerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New response variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['overestimation'] = (data['logerror'] >= 0).astype(int)\n",
    "data['extreme_overestimation'] = (data['logerror'] >= 1.5).astype(int)\n",
    "data['extreme_underestimation'] = (data['logerror'] <= -1.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup variables considered in the model\n",
    "\n",
    "\n",
    "# numerical variables\n",
    "num_atts = ['bathroomcnt','bedroomcnt','buildingqualitytypeid','calculatedbathnbr','finishedfloor1squarefeet',\n",
    "           'calculatedfinishedsquarefeet','finishedsquarefeet12','finishedsquarefeet13',\n",
    "           'finishedsquarefeet15','finishedsquarefeet50','finishedsquarefeet6','fireplacecnt',\n",
    "           'fullbathcnt','garagecarcnt','garagetotalsqft','latitude','longitude','lotsizesquarefeet',\n",
    "           'poolcnt','poolsizesum','censustractandblock','roomcnt','threequarterbathnbr','unitcnt',\n",
    "           'yardbuildingsqft17','yardbuildingsqft26','numberofstories',\n",
    "            'structuretaxvaluedollarcnt','taxvaluedollarcnt','landtaxvaluedollarcnt','taxamount',\n",
    "           'N-ValueRatio', 'N-LivingAreaProp', 'N-ValueProp']\n",
    "\n",
    "# categorical varaibles\n",
    "cat_atts = ['airconditioningtypeid','architecturalstyletypeid',\n",
    "           'buildingclasstypeid','heatingorsystemtypeid','pooltypeid10','pooltypeid2',\n",
    "            'pooltypeid7','propertylandusetypeid','regionidcounty',\n",
    "           'storytypeid','typeconstructiontypeid','yearbuilt','fireplaceflag',\n",
    "           'taxdelinquencyflag']\n",
    "\n",
    "# Dictionary of categorical variables and their default levels\n",
    "cat_dict = {key:value for key,value in {'airconditioningtypeid':[-1] + list(range(1,14)),\n",
    "           'architecturalstyletypeid':[-1] + list(range(1,28)),\n",
    "           'buildingclasstypeid':[-1] + list(range(1,6)),\n",
    "            'heatingorsystemtypeid':[-1] + list(range(1,26)),\n",
    "            'pooltypeid10': list(range(-1,2)),\n",
    "            'pooltypeid2': list(range(-1,2)),\n",
    "            'pooltypeid7': list(range(-1,2)),\n",
    "            'Pool': [0,1],\n",
    "            'propertylandusetypeid': [-1, 31,46,47,246,247,248,260,261,262,263,264,265,266,267,268,269,270,271,\n",
    "                                     273,274,275,276,279,290,291],\n",
    "            'regionidcounty': [2061,3101,1286],\n",
    "            'storytypeid':[-1] + list(range(1,36)),\n",
    "            'typeconstructiontypeid':[-1] + list(range(1,19)),\n",
    "            'yearbuilt': [-1] + list(range(1885,2018)),\n",
    "            'fireplaceflag': [-1] + ['True','False'],\n",
    "            'taxdelinquencyflag': [-1] + ['Y','N']\n",
    "           }.items() if key in cat_atts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "        ('select_and_dummify', pipes.DF_Selector_GetDummies(cat_dict)),\n",
    "    ])\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', pipes.DataFrameSelector(num_atts)),\n",
    "        ('imputer', Imputer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# Full pipeline\n",
    "feature_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into the 10-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2199, 86155, 84691, ..., 86952, 82677, 76398])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(19)\n",
    "np.random.shuffle(indices) # in-place shuffling \n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_indices = {(i+1):indices[i::10] for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 2199, 83721, 29492, ..., 37852, 48911, 39220]),\n",
       " 2: array([86155, 32252, 81949, ..., 57319, 13479, 33811]),\n",
       " 3: array([84691, 37597,  3215, ..., 84821, 43372, 86952]),\n",
       " 4: array([11172, 67082, 58364, ..., 74500, 63830, 82677]),\n",
       " 5: array([78769, 73075, 17232, ..., 12489,   266, 76398]),\n",
       " 6: array([53035, 17238, 32604, ..., 14649, 26827, 61025]),\n",
       " 7: array([58194, 72307,  3380, ..., 57397, 68361, 53125]),\n",
       " 8: array([66378, 81551, 66156, ..., 73922, 85799, 45218]),\n",
       " 9: array([70318, 70507, 20646, ...,  7537, 69584, 17218]),\n",
       " 10: array([42552, 66817, 57336, ..., 88913, 67815, 17738])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1 Classification - P(overestimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models on the 10 splits of data \\ fold_i for i = 1,...,10 & obtaining level 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(desired_cols=['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'fi... 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"network\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 50), random_state=1)),\n",
    "    (\"logistic\", LogisticRegression(penalty='l1', tol=0.01)),\n",
    "    (\"rf_1\",RandomForestClassifier(max_depth=8, random_state=9,class_weight={0:0.52,1:0.48})),\n",
    "    (\"rf_2\", RandomForestClassifier(max_depth=12, random_state=9)),\n",
    "    (\"rf_3\", RandomForestClassifier(max_depth=12, random_state=9, class_weight={0:0.55,1:0.45})),\n",
    "    (\"xgb_1\", XGBClassifier(max_depth=7, random_state=9, n_jobs=2)),\n",
    "    (\"xgb_2\", XGBClassifier(max_depth=5, random_state=9, n_jobs=2)),\n",
    "    (\"lgbm_1\", LGBMClassifier(random_state=9, n_jobs=2, n_estimators=100)),\n",
    "    (\"lgbm_2\", LGBMClassifier(random_state=9, n_jobs=2, n_estimators=100, boosting_type = 'goss'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: network\n",
      "...working on fold 1\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 2\n",
      "......training model\n",
      "......obtaining level-one data\n",
      "...working on fold 3\n",
      "......training model\n"
     ]
    }
   ],
   "source": [
    "level_one_data = data[['parcelid']].copy()\n",
    "\n",
    "for pair in models:\n",
    "    current_model_name,current_model = pair\n",
    "    print(\"Current model: %s\" % current_model_name)\n",
    "    \n",
    "    # initialize an NoneObject to be a placeholder for level-one data for current model\n",
    "    model_preds = None \n",
    "    \n",
    "    for fold_nbr in range(1,11):\n",
    "        print(\"...working on fold %d\" % fold_nbr)\n",
    "\n",
    "        # set training data X \\ fold\n",
    "        current_traindata = data.iloc[np.setdiff1d(indices,fold_indices[fold_nbr]),]\n",
    "\n",
    "        # get a clone of the model and fit the current training data\n",
    "        print('......training model')\n",
    "        clf = clone(current_model)\n",
    "        clf.fit(feature_pipeline.transform(current_traindata), current_traindata['overestimation'])\n",
    "\n",
    "        # level-one data (i.e., predict observations on current fold using reg)\n",
    "        print('......obtaining level-one data')\n",
    "        fold_data = data.iloc[fold_indices[fold_nbr]]\n",
    "        fold_preds = Series(clf.predict_proba(feature_pipeline.transform(fold_data))[:,1], \n",
    "                            index=fold_indices[fold_nbr], name = current_model_name)\n",
    "\n",
    "        # adding to the placeholder for level-one data\n",
    "        if model_preds is not None:\n",
    "            model_preds = pd.concat([model_preds, fold_preds])\n",
    "        else:\n",
    "            model_preds = fold_preds\n",
    "\n",
    "        # some housecleaning\n",
    "        del clf\n",
    "    \n",
    "    # add level-one predictions of current model to running dataframe\n",
    "    level_one_data = pd.concat([level_one_data, model_preds], axis=1)\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level_one_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level_one_data.shape[0]==data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level_one_data.to_csv(\"/home/anerdi/Desktop/Zillow/levelonedata/stage1_l1data.csv.gz\", compression ='gzip',\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.merge(level_one_data, data[['overestimation','parcelid']], on='parcelid')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked = LogisticRegression(penalty='l1', tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked.fit(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']], training_data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked.score(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']], training_data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = stacked.predict(training_data[['network','rf_2','rf_3','xgb_1','xgb_2','lgbm_1','lgbm_2']])\n",
    "true = training_data['overestimation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(recall_score(true,pred))\n",
    "print(precision_score(true,pred))\n",
    "print(f1_score(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(true, pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_ann = clone(models[0][1])\n",
    "clf_rf2 = clone(models[3][1])\n",
    "clf_rf3 = clone(models[4][1])\n",
    "clf_xgb1 = clone(models[5][1])\n",
    "clf_xgb2 = clone(models[6][1])\n",
    "clf_lgbm1 = clone(models[7][1])\n",
    "clf_lgbm2 = clone(models[8][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_ann.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_rf2.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_rf3.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_xgb1.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_xgb2.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_lgbm1.fit(feature_pipeline.transform(data), data['overestimation'])\n",
    "clf_lgbm2.fit(feature_pipeline.transform(data), data['overestimation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [('ann',clf_ann),\n",
    "          ('rf2',clf_rf2),\n",
    "          ('rf3',clf_rf3),\n",
    "          ('xgb1',clf_xgb1),\n",
    "          ('xgb2',clf_xgb2),\n",
    "          ('lgbm1',clf_lgbm1),\n",
    "          ('lgbm2',clf_lgbm2)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overestimate_probs = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])\n",
    "for pair in models:\n",
    "    model_name, model = pair\n",
    "    probabilities = None\n",
    "    for i in range(int(properties.shape[0] / 100000)):   \n",
    "        # get current test features\n",
    "        current_test_feats = feature_pipeline.transform(properties.iloc[i*100000:(i+1)*100000])\n",
    "\n",
    "        # predict on current test obs\n",
    "        current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='%s_overestimate_prob'%model_name,\n",
    "                              index = np.arange(i*100000,(i+1)*100000))\n",
    "\n",
    "        if probabilities is not None:\n",
    "            probabilities = pd.concat([probabilities, current_probs])\n",
    "        else:\n",
    "            probabilities = current_probs\n",
    "\n",
    "    #  fencepost problem\n",
    "    current_test_feats = feature_pipeline.transform(properties.iloc[2900000:])\n",
    "    current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='%s_overestimate_prob'%model_name,\n",
    "                          index = np.arange(2900000,2985217))\n",
    "    probabilities = pd.concat([probabilities, current_probs])\n",
    "    overestimate_probs = pd.concat([overestimate_probs, probabilities], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overestimate_probs['stacked_pred'] = 1 / (1 + np.exp(-stacked.intercept_[0]\n",
    "         - overestimate_probs['ann_overestimate_prob']*stacked.coef_[0][0]\n",
    "            - overestimate_probs['rf2_overestimate_prob']*stacked.coef_[0][1]\n",
    "                - overestimate_probs['rf3_overestimate_prob']*stacked.coef_[0][2]\n",
    "                     - overestimate_probs['xgb1_overestimate_prob']*stacked.coef_[0][3]\n",
    "                        - overestimate_probs['xgb2_overestimate_prob']*stacked.coef_[0][4]\n",
    "                            - overestimate_probs['lgbm1_overestimate_prob']*stacked.coef_[0][5]\n",
    "                                - overestimate_probs['lgbm2_overestimate_prob']*stacked.coef_[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overestimate_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overestimate_probs.to_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs_stacked_ann_rfs_xgbs_lgbms.csv.gz\", \n",
    "                          index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Layer 2 P(extreme_overestimation | overestimation) and P(extreme_underestimation | underestimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_over = feature_pipeline.fit_transform(data[data['overestimation'] == 1])\n",
    "y_extreme_over = data[data['overestimation'] == 1]['extreme_overestimation']\n",
    "X_under = feature_pipeline.fit_transform(data[data['overestimation'] == 0])\n",
    "y_extreme_under = data[data['overestimation'] == 0]['extreme_underestimation']\n",
    "\n",
    "assert X_over.shape[0] == y_extreme_over.shape[0]\n",
    "assert X_under.shape[0] == y_extreme_under.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=15, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=9,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_layer2_over = RandomForestClassifier(max_depth=15, random_state=9, class_weight='balanced')\n",
    "clf_rf_layer2_over.fit(X_over, y_extreme_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=12, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=9,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_layer2_under = RandomForestClassifier(max_depth=12, random_state=9, class_weight='balanced')\n",
    "clf_rf_layer2_under.fit(X_under, y_extreme_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50436,    33],\n",
       "       [   70,    69]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(clf_rf_layer2_over.predict(X_over), y_extreme_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39520,    17],\n",
       "       [   88,    42]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(clf_rf_layer2_under.predict(X_under), y_extreme_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [('over',clf_rf_layer2_over),('under',clf_rf_layer2_under)]\n",
    "extreme_probs = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])\n",
    "for pair in models:\n",
    "    model_name, model = pair\n",
    "    probabilities = None\n",
    "    for i in range(int(properties.shape[0] / 100000)):   \n",
    "        # get current test features\n",
    "        current_test_feats = feature_pipeline.transform(properties.iloc[i*100000:(i+1)*100000])\n",
    "\n",
    "        # predict on current test obs\n",
    "        current_probs = DataFrame(model.predict_proba(current_test_feats), name='extreme_%s_prob'%model_name,\n",
    "                              index = np.arange(i*100000,(i+1)*100000))\n",
    "\n",
    "        if probabilities is not None:\n",
    "            probabilities = pd.concat([probabilities, current_probs])\n",
    "        else:\n",
    "            probabilities = current_probs\n",
    "\n",
    "    #  fencepost problem\n",
    "    current_test_feats = feature_pipeline.transform(properties.iloc[2900000:])\n",
    "    current_probs = Series(model.predict_proba(current_test_feats)[:,1], name='extreme_%s_prob'%model_name,\n",
    "                          index = np.arange(2900000,2985217))\n",
    "    probabilities = pd.concat([probabilities, current_probs])\n",
    "    extreme_probs = pd.concat([extreme_probs, probabilities], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>extreme_over_prob</th>\n",
       "      <th>extreme_under_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.544759</td>\n",
       "      <td>0.192589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.283555</td>\n",
       "      <td>0.220792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.124701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.184405</td>\n",
       "      <td>0.136827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.238265</td>\n",
       "      <td>0.059137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  extreme_over_prob  extreme_under_prob\n",
       "0  10754147           0.544759            0.192589\n",
       "1  10759547           0.283555            0.220792\n",
       "2  10843547           0.357077            0.124701\n",
       "3  10859147           0.184405            0.136827\n",
       "4  10879947           0.238265            0.059137"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (overestimate_probs['parcelid'] == extreme_probs['parcelid']).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs = pd.merge(overestimate_probs[['parcelid','stacked_pred']], extreme_probs, on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs['over_but_not_extreme'] = overall_probs['stacked_pred']*(1 - overall_probs['extreme_over_prob'])\n",
    "overall_probs['over_and_extreme'] = overall_probs['stacked_pred']*overall_probs['extreme_over_prob']\n",
    "overall_probs['under_but_not_extreme'] = (1 - overall_probs['stacked_pred'])*(1 - overall_probs['extreme_under_prob'])\n",
    "overall_probs['under_and_extreme'] = (1 - overall_probs['stacked_pred'])*overall_probs['extreme_under_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>stacked_pred</th>\n",
       "      <th>extreme_over_prob</th>\n",
       "      <th>extreme_under_prob</th>\n",
       "      <th>over_but_not_extreme</th>\n",
       "      <th>over_and_extreme</th>\n",
       "      <th>under_but_not_extreme</th>\n",
       "      <th>under_and_extreme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.473844</td>\n",
       "      <td>0.544759</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.215713</td>\n",
       "      <td>0.258131</td>\n",
       "      <td>0.424824</td>\n",
       "      <td>0.101332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.474768</td>\n",
       "      <td>0.283555</td>\n",
       "      <td>0.220792</td>\n",
       "      <td>0.340146</td>\n",
       "      <td>0.134623</td>\n",
       "      <td>0.409265</td>\n",
       "      <td>0.115967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.732197</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.124701</td>\n",
       "      <td>0.470746</td>\n",
       "      <td>0.261451</td>\n",
       "      <td>0.234407</td>\n",
       "      <td>0.033395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.606960</td>\n",
       "      <td>0.184405</td>\n",
       "      <td>0.136827</td>\n",
       "      <td>0.495034</td>\n",
       "      <td>0.111926</td>\n",
       "      <td>0.339262</td>\n",
       "      <td>0.053778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.529348</td>\n",
       "      <td>0.238265</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>0.403223</td>\n",
       "      <td>0.126125</td>\n",
       "      <td>0.442819</td>\n",
       "      <td>0.027833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  stacked_pred  extreme_over_prob  extreme_under_prob  \\\n",
       "0  10754147      0.473844           0.544759            0.192589   \n",
       "1  10759547      0.474768           0.283555            0.220792   \n",
       "2  10843547      0.732197           0.357077            0.124701   \n",
       "3  10859147      0.606960           0.184405            0.136827   \n",
       "4  10879947      0.529348           0.238265            0.059137   \n",
       "\n",
       "   over_but_not_extreme  over_and_extreme  under_but_not_extreme  \\\n",
       "0              0.215713          0.258131               0.424824   \n",
       "1              0.340146          0.134623               0.409265   \n",
       "2              0.470746          0.261451               0.234407   \n",
       "3              0.495034          0.111926               0.339262   \n",
       "4              0.403223          0.126125               0.442819   \n",
       "\n",
       "   under_and_extreme  \n",
       "0           0.101332  \n",
       "1           0.115967  \n",
       "2           0.033395  \n",
       "3           0.053778  \n",
       "4           0.027833  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2985217.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_probs[['over_but_not_extreme','over_and_extreme','under_but_not_extreme','under_and_extreme']].sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_probs.to_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/two_layer_probabilities.csv.gz\", \n",
    "                          index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
