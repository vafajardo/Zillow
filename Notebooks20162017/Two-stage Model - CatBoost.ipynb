{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import feature_pipelines as pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_regression_preds(reg,X_test, model_name='pred_logerror', transactiondate='2016-12-01'):\n",
    "    X_test['transactiondate'] = pd.Timestamp(transactiondate)  # Dummy\n",
    "    X_test = add_date_features(X_test)\n",
    "    reg_preds = None\n",
    "    for i in range(int(properties.shape[0] / 100000)):   \n",
    "        # get current test features\n",
    "        current_test_feats = X_test.iloc[i*100000:(i+1)*100000]\n",
    "\n",
    "        # predict on current test obs\n",
    "        current_preds = Series(reg.predict(current_test_feats), name=model_name,\n",
    "                              index = np.arange(i*100000,(i+1)*100000))\n",
    "\n",
    "        if reg_preds is not None:\n",
    "            reg_preds = pd.concat([reg_preds, current_preds])\n",
    "        else:\n",
    "            reg_preds = current_preds\n",
    "\n",
    "    #  fencepost problem\n",
    "    current_test_feats = X_test.iloc[2900000:]\n",
    "    current_preds = Series(reg.predict(current_test_feats), name=model_name,\n",
    "                          index = np.arange(2900000,2985217))\n",
    "    reg_preds = pd.concat([reg_preds, current_preds])\n",
    "    return reg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_errors(submission_df, comparison_df):\n",
    "    \"\"\"\n",
    "    This function takes a submission entry for public leaderboard, and returns\n",
    "    the training error for each month.\n",
    "    \"\"\"\n",
    "    # training error\n",
    "    trainresults = pd.merge(submission_df[['ParcelId','201610','201611','201612']], comparison_df[['parcelid','logerror','month']],\n",
    "                           left_on='ParcelId', right_on='parcelid')\n",
    "    oct_error = abs(trainresults[trainresults['month'] == 10]['201610'] \n",
    "                    - trainresults[trainresults['month'] == 10]['logerror']).mean()\n",
    "    nov_error = abs(trainresults[trainresults['month'] == 11]['201611'] \n",
    "                    - trainresults[trainresults['month'] == 11]['logerror']).mean()\n",
    "    dec_error = abs(trainresults[trainresults['month'] == 12]['201612'] \n",
    "                    - trainresults[trainresults['month'] == 12]['logerror']).mean()\n",
    "    overall_mae = (oct_error*(trainresults['month'] == 10).sum() + nov_error*(trainresults['month'] == 11).sum() \n",
    "                        + dec_error*(trainresults['month'] == 12).sum()) / (trainresults['month'].isin([10,11,12])).sum()\n",
    "    return (oct_error, nov_error, dec_error, overall_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "\n",
    "# train_df = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\", parse_dates=['transactiondate'], low_memory=False)\n",
    "train_df = pd.read_csv(\"/home/anerdi/Desktop/Zillow/data/traindata20162017.csv.gz\", parse_dates=['transactiondate'], low_memory=False)\n",
    "test_df = pd.read_csv(maindir + \"/data/sample_submission.csv\", low_memory=False)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", low_memory=False)\n",
    "# field is named differently in submission\n",
    "test_df['parcelid'] = test_df['ParcelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data  Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similar to the1owl\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (167888, 65)\n",
      "Test:  (2985217, 65)\n"
     ]
    }
   ],
   "source": [
    "train_df = add_date_features(train_df)\n",
    "# train_df = train_df.merge(properties, how='left', on='parcelid')\n",
    "test_df = test_df.merge(properties, how='left', on='parcelid')\n",
    "print(\"Train: \", train_df.shape)\n",
    "print(\"Test: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a) Remove missing data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We exclude: ['architecturalstyletypeid', 'basementsqft', 'buildingclasstypeid', 'decktypeid', 'finishedsquarefeet13', 'finishedsquarefeet6', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'storytypeid', 'typeconstructiontypeid', 'yardbuildingsqft26', 'fireplaceflag']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % exclude_missing)\n",
    "print(len(exclude_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b) Remove data that is always the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We exclude: ['decktypeid', 'hashottuborspa', 'poolcnt', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'storytypeid', 'fireplaceflag', 'taxdelinquencyflag']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# exclude where we only have one unique value :D\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % exclude_unique)\n",
    "print(len(exclude_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a) Define training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use these for training: ['airconditioningtypeid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet15', 'finishedsquarefeet50', 'fips', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet', 'propertycountylandusecode', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yardbuildingsqft17', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt', 'taxamount', 'taxdelinquencyyear', 'censustractandblock', 'transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter']\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "exclude_other = ['parcelid', 'logerror','year','month']  # for indexing/training only\n",
    "# do not know what this is LARS, 'SHCG' 'COR2YY' 'LNR2RPD-R3' ?!?\n",
    "exclude_other.append('propertyzoningdesc')\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % train_features)\n",
    "print(len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Define which of these training features are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat features are: ['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'taxdelinquencyyear', 'transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter']\n"
     ]
    }
   ],
   "source": [
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some out of range int is a good choice\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167888, 43) (167888,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix_overestimated = np.where(y_train >= 0)[0]\n",
    "ix_underestimated = np.where(y_train < 0)[0]\n",
    "data_indices = {\"over\": ix_overestimated, \"under\": ix_underestimated}\n",
    "data_indices = {\"over\": ix_overestimated, \"under\": ix_underestimated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert ix_overestimated.shape[0] + ix_underestimated.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94505,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_overestimated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73383,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_underestimated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985217, 43)\n"
     ]
    }
   ],
   "source": [
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n",
    "test_df = add_date_features(test_df)\n",
    "X_test = test_df[train_features]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"catboost1\", CatBoostRegressor(\n",
    "                iterations=200, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE')\n",
    "    ),\n",
    "    (\"catboost2\", CatBoostRegressor(\n",
    "                iterations=200, learning_rate=0.05,\n",
    "                depth=5, l2_leaf_reg=5,\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE')\n",
    "    ),\n",
    "    (\"catboost3\", CatBoostRegressor(\n",
    "                iterations=200, learning_rate=0.05,\n",
    "                depth=5, l2_leaf_reg=7,\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE')\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: catboost2\n",
      "1\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n"
     ]
    }
   ],
   "source": [
    "test_predictions_one_stage = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])\n",
    "\n",
    "for pair in models:\n",
    "    current_model_name,current_model = pair\n",
    "    print(\"Current model: %s\" % current_model_name)\n",
    "    \n",
    "    num_ensembles = 5\n",
    "    y_pred_oct = Series(np.zeros(2985217), name = \"{0}_201610\".format(current_model_name))\n",
    "    y_pred_nov = Series(np.zeros(2985217), name = \"{0}_201611\".format(current_model_name))\n",
    "    y_pred_dec = Series(np.zeros(2985217), name = \"{0}_201612\".format(current_model_name))\n",
    "    for i in range(num_ensembles):            \n",
    "        # get a clone of the model and fit the current training data\n",
    "        print(i+1)\n",
    "        reg = clone(current_model)\n",
    "        reg.set_params(random_seed=i)\n",
    "\n",
    "        print(\"...fitting model\")\n",
    "        reg.fit(X_train, y_train,cat_features=cat_feature_inds)\n",
    "\n",
    "        print(\"...obtaining predictions on test set\")\n",
    "        # obtain predictions on test set  \n",
    "        y_pred_oct = y_pred_oct + generate_regression_preds(reg, X_test, model_name=\"{0}_201610\".format(current_model_name),\n",
    "                                                            transactiondate='2016-10-01')\n",
    "        y_pred_nov = y_pred_nov + generate_regression_preds(reg, X_test, model_name=\"{0}_201611\".format(current_model_name), \n",
    "                                                            transactiondate='2016-11-01')\n",
    "        y_pred_dec = y_pred_dec + generate_regression_preds(reg, X_test, model_name=\"{0}_201612\".format(current_model_name),\n",
    "                                                            transactiondate='2016-12-01')\n",
    "\n",
    "    # model averaging    \n",
    "    y_pred_oct = y_pred_oct / num_ensembles\n",
    "    y_pred_nov = y_pred_nov / num_ensembles\n",
    "    y_pred_dec = y_pred_dec / num_ensembles\n",
    "    test_predictions_one_stage = pd.concat([test_predictions_one_stage, y_pred_oct, y_pred_nov, y_pred_dec], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_feats_importance = sorted([(-reg.feature_importances_[ix], train_features[ix]) for ix in cat_feature_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-7.6865824404776255, 'transaction_month'),\n",
       " (-7.294274950257516, 'propertycountylandusecode'),\n",
       " (-5.818644878328299, 'regionidzip'),\n",
       " (-3.6599379203024918, 'yearbuilt'),\n",
       " (-3.403491862177784, 'propertylandusetypeid'),\n",
       " (-3.1541412521002385, 'regionidneighborhood'),\n",
       " (-2.897418110866512, 'regionidcity'),\n",
       " (-1.9124589847592146, 'taxdelinquencyyear'),\n",
       " (-1.8406540308430321, 'buildingqualitytypeid'),\n",
       " (-1.6357444772830403, 'transaction_quarter'),\n",
       " (-1.5647768242706777, 'heatingorsystemtypeid'),\n",
       " (-1.4587876368194137, 'transaction_day'),\n",
       " (-1.2330244313233854, 'regionidcounty'),\n",
       " (-1.0268083373933814, 'airconditioningtypeid'),\n",
       " (-0.9556557314257367, 'transaction_year'),\n",
       " (-0.8917645807364831, 'assessmentyear'),\n",
       " (-0.5437287387925018, 'fips')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feats_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'catboost3'\n",
    "new_submission_one_stage = DataFrame({'ParcelId': test_predictions_one_stage['parcelid'],\n",
    "                           '201610':test_predictions_one_stage['%s_201610' % model_name],\n",
    "                           '201611':test_predictions_one_stage['%s_201611' % model_name],\n",
    "                           '201612':test_predictions_one_stage['%s_201612' % model_name],\n",
    "})\n",
    "new_submission_one_stage['201710'] = 0\n",
    "new_submission_one_stage['201711'] = 0\n",
    "new_submission_one_stage['201712'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0615890496282902,\n",
       " 0.059950000000000024,\n",
       " 0.07292461184588844,\n",
       " 0.063546394287052249)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_errors(new_submission_one_stage.round(4), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06160429977898345,\n",
       " 0.06000580503833507,\n",
       " 0.07284450833812528,\n",
       " 0.063550901428236994)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best\n",
    "mean_absolute_errors(new_submission_one_stage.round(4), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_submission_one_stage.round(4).to_csv(\"/home/anerdi/Desktop/Zillow/submissions/catboost3.csv.gz\",\n",
    "                     compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: catboost\n",
      "over_10\n",
      "1\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "2\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "3\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "4\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "5\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "under_10\n",
      "1\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "2\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "3\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "4\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n",
      "5\n",
      "...fitting model\n",
      "...obtaining predictions on test set\n"
     ]
    }
   ],
   "source": [
    "test_predictions = pd.read_csv(maindir + \"/data/properties_2016_with_2017_tax.csv\", usecols=['parcelid'])\n",
    "\n",
    "for pair in models:\n",
    "    current_model_name,current_model = pair\n",
    "    print(\"Current model: %s\" % current_model_name)\n",
    "    \n",
    "    for key,val in data_indices.items():\n",
    "        type_of_zestimate, ix = key, val\n",
    "\n",
    "        # preprocess current training data\n",
    "        current_train = X_train.iloc[ix,]\n",
    "        current_y = y_train.iloc[ix,]\n",
    "\n",
    "        for month in [10]:            \n",
    "            print(\"%s_%d\" % (type_of_zestimate, month))\n",
    "            \n",
    "            num_ensembles = 5\n",
    "            y_pred = Series(np.zeros(2985217), name = \"%s_%d_%s\" % (current_model_name, month,type_of_zestimate))\n",
    "            for i in range(num_ensembles):            \n",
    "                # get a clone of the model and fit the current training data\n",
    "                print(i+1)\n",
    "                reg = clone(current_model)\n",
    "                reg.set_params(random_seed=i)\n",
    "                print(\"...fitting model\")\n",
    "                reg.fit(current_train, current_y,cat_features=cat_feature_inds)\n",
    "                print(\"...obtaining predictions on test set\")\n",
    "                # obtain predictions on test set\n",
    "                y_pred = y_pred + generate_regression_preds(reg, model_name=\"%s_%d_%s\" % (current_model_name,\n",
    "                                                             month,type_of_zestimate), month = month)\n",
    "            y_pred = y_pred / num_ensembles\n",
    "            test_predictions = pd.concat([test_predictions, y_pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>catboost_10_over</th>\n",
       "      <th>catboost_10_under</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>-0.071177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.085959</td>\n",
       "      <td>-0.071010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.093681</td>\n",
       "      <td>-0.111239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.078073</td>\n",
       "      <td>-0.096214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.062657</td>\n",
       "      <td>-0.090571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  catboost_10_over  catboost_10_under\n",
       "0  10754147          0.086996          -0.071177\n",
       "1  10759547          0.085959          -0.071010\n",
       "2  10843547          0.093681          -0.111239\n",
       "3  10859147          0.078073          -0.096214\n",
       "4  10879947          0.062657          -0.090571"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions.to_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/catboost-two-stage-preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overestimate_probabilities = pd.read_csv(\"/home/anerdi/Desktop/Zillow/twostagemodel/overestimate_probs_stacked_ann_rfs_xgbs_lgbms_20162017.csv.gz\")\n",
    "overestimate_probabilities.rename(columns={'stacked_pred':'overestimate_prob'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>ann_overestimate_prob</th>\n",
       "      <th>rf2_overestimate_prob</th>\n",
       "      <th>rf3_overestimate_prob</th>\n",
       "      <th>xgb1_overestimate_prob</th>\n",
       "      <th>xgb2_overestimate_prob</th>\n",
       "      <th>lgbm1_overestimate_prob</th>\n",
       "      <th>lgbm2_overestimate_prob</th>\n",
       "      <th>overestimate_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.466125</td>\n",
       "      <td>0.497368</td>\n",
       "      <td>0.560124</td>\n",
       "      <td>0.462231</td>\n",
       "      <td>0.479722</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.650032</td>\n",
       "      <td>0.515646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.412405</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>0.420662</td>\n",
       "      <td>0.525333</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.543325</td>\n",
       "      <td>0.646905</td>\n",
       "      <td>0.553857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.527465</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.535855</td>\n",
       "      <td>0.411728</td>\n",
       "      <td>0.522140</td>\n",
       "      <td>0.544722</td>\n",
       "      <td>0.501281</td>\n",
       "      <td>0.466540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.665014</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.456073</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.570832</td>\n",
       "      <td>0.590595</td>\n",
       "      <td>0.576605</td>\n",
       "      <td>0.581211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.477125</td>\n",
       "      <td>0.524182</td>\n",
       "      <td>0.483156</td>\n",
       "      <td>0.514524</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.515008</td>\n",
       "      <td>0.404796</td>\n",
       "      <td>0.483875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  ann_overestimate_prob  rf2_overestimate_prob  \\\n",
       "0  10754147               0.466125               0.497368   \n",
       "1  10759547               0.412405               0.500739   \n",
       "2  10843547               0.527465               0.636537   \n",
       "3  10859147               0.665014               0.648656   \n",
       "4  10879947               0.477125               0.524182   \n",
       "\n",
       "   rf3_overestimate_prob  xgb1_overestimate_prob  xgb2_overestimate_prob  \\\n",
       "0               0.560124                0.462231                0.479722   \n",
       "1               0.420662                0.525333                0.527149   \n",
       "2               0.535855                0.411728                0.522140   \n",
       "3               0.456073                0.580416                0.570832   \n",
       "4               0.483156                0.514524                0.509930   \n",
       "\n",
       "   lgbm1_overestimate_prob  lgbm2_overestimate_prob  overestimate_prob  \n",
       "0                 0.511108                 0.650032           0.515646  \n",
       "1                 0.543325                 0.646905           0.553857  \n",
       "2                 0.544722                 0.501281           0.466540  \n",
       "3                 0.590595                 0.576605           0.581211  \n",
       "4                 0.515008                 0.404796           0.483875  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overestimate_probabilities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = pd.merge(test_predictions, overestimate_probabilities, on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pair in models:\n",
    "    current_model_name, current_model = pair\n",
    "    # combine over and under to get prediction\n",
    "    for month in [10]:\n",
    "        test_predictions['{0}_{1}'.format(current_model_name, month)] = (test_predictions['%s_%d_over' % (current_model_name, month)]*test_predictions['overestimate_prob'] \n",
    "                + test_predictions['%s_%d_under' % (current_model_name, month)]*(1 - test_predictions['overestimate_prob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>catboost_10_over</th>\n",
       "      <th>catboost_10_under</th>\n",
       "      <th>ann_overestimate_prob</th>\n",
       "      <th>rf2_overestimate_prob</th>\n",
       "      <th>rf3_overestimate_prob</th>\n",
       "      <th>xgb1_overestimate_prob</th>\n",
       "      <th>xgb2_overestimate_prob</th>\n",
       "      <th>lgbm1_overestimate_prob</th>\n",
       "      <th>lgbm2_overestimate_prob</th>\n",
       "      <th>overestimate_prob</th>\n",
       "      <th>catboost_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>-0.071177</td>\n",
       "      <td>0.466125</td>\n",
       "      <td>0.497368</td>\n",
       "      <td>0.560124</td>\n",
       "      <td>0.462231</td>\n",
       "      <td>0.479722</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.650032</td>\n",
       "      <td>0.515646</td>\n",
       "      <td>0.010384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.085959</td>\n",
       "      <td>-0.071010</td>\n",
       "      <td>0.412405</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>0.420662</td>\n",
       "      <td>0.525333</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.543325</td>\n",
       "      <td>0.646905</td>\n",
       "      <td>0.553857</td>\n",
       "      <td>0.015928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.093681</td>\n",
       "      <td>-0.111239</td>\n",
       "      <td>0.527465</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.535855</td>\n",
       "      <td>0.411728</td>\n",
       "      <td>0.522140</td>\n",
       "      <td>0.544722</td>\n",
       "      <td>0.501281</td>\n",
       "      <td>0.466540</td>\n",
       "      <td>-0.015636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.078073</td>\n",
       "      <td>-0.096214</td>\n",
       "      <td>0.665014</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.456073</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.570832</td>\n",
       "      <td>0.590595</td>\n",
       "      <td>0.576605</td>\n",
       "      <td>0.581211</td>\n",
       "      <td>0.005083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.062657</td>\n",
       "      <td>-0.090571</td>\n",
       "      <td>0.477125</td>\n",
       "      <td>0.524182</td>\n",
       "      <td>0.483156</td>\n",
       "      <td>0.514524</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.515008</td>\n",
       "      <td>0.404796</td>\n",
       "      <td>0.483875</td>\n",
       "      <td>-0.016428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  catboost_10_over  catboost_10_under  ann_overestimate_prob  \\\n",
       "0  10754147          0.086996          -0.071177               0.466125   \n",
       "1  10759547          0.085959          -0.071010               0.412405   \n",
       "2  10843547          0.093681          -0.111239               0.527465   \n",
       "3  10859147          0.078073          -0.096214               0.665014   \n",
       "4  10879947          0.062657          -0.090571               0.477125   \n",
       "\n",
       "   rf2_overestimate_prob  rf3_overestimate_prob  xgb1_overestimate_prob  \\\n",
       "0               0.497368               0.560124                0.462231   \n",
       "1               0.500739               0.420662                0.525333   \n",
       "2               0.636537               0.535855                0.411728   \n",
       "3               0.648656               0.456073                0.580416   \n",
       "4               0.524182               0.483156                0.514524   \n",
       "\n",
       "   xgb2_overestimate_prob  lgbm1_overestimate_prob  lgbm2_overestimate_prob  \\\n",
       "0                0.479722                 0.511108                 0.650032   \n",
       "1                0.527149                 0.543325                 0.646905   \n",
       "2                0.522140                 0.544722                 0.501281   \n",
       "3                0.570832                 0.590595                 0.576605   \n",
       "4                0.509930                 0.515008                 0.404796   \n",
       "\n",
       "   overestimate_prob  catboost_10  \n",
       "0           0.515646     0.010384  \n",
       "1           0.553857     0.015928  \n",
       "2           0.466540    -0.015636  \n",
       "3           0.581211     0.005083  \n",
       "4           0.483875    -0.016428  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'catboost'\n",
    "new_submission = DataFrame({'ParcelId': test_predictions['parcelid'],\n",
    "                           '201610':test_predictions['%s_10' % model_name],\n",
    "                           '201611':test_predictions['%s_10' % model_name],\n",
    "                           '201612':test_predictions['%s_10' % model_name],\n",
    "})\n",
    "new_submission['201710'] = 0\n",
    "new_submission['201711'] = 0\n",
    "new_submission['201712'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>ParcelId</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010384</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>10754147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>10759547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015636</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>10843547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>10859147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.016428</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>10879947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     201610    201611    201612  ParcelId  201710  201711  201712\n",
       "0  0.010384  0.010384  0.010384  10754147       0       0       0\n",
       "1  0.015928  0.015928  0.015928  10759547       0       0       0\n",
       "2 -0.015636 -0.015636 -0.015636  10843547       0       0       0\n",
       "3  0.005083  0.005083  0.005083  10859147       0       0       0\n",
       "4 -0.016428 -0.016428 -0.016428  10879947       0       0       0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['month'] = train_df['transaction_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06220480208961224,\n",
       " 0.06115547645125963,\n",
       " 0.07376003450258775,\n",
       " 0.064332931397799151)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two-stage with xgb\n",
    "mean_absolute_errors(new_submission.round(4), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06168199718706048,\n",
       " 0.060239156626506224,\n",
       " 0.07304933870040252,\n",
       " 0.063687754624209822)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best\n",
    "mean_absolute_errors(new_submission_one_stage.round(4), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06174942736588318,\n",
       " 0.06084288061336259,\n",
       " 0.07342547441058092,\n",
       " 0.063932673846874372)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best\n",
    "mean_absolute_errors(new_submission.round(4), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_submission.round(4).to_csv(\"/home/anerdi/Desktop/Zillow/submissions/two_stage_stage1_stacked_annrfsxgbs_stage2_catboost_201617.csv.gz\", index=False,\n",
    "                     compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
