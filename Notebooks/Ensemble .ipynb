{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_absolute_errors(submission_df, comparison_df):\n",
    "    \"\"\"\n",
    "    This function takes a submission entry for public leaderboard, and returns\n",
    "    the training error for each month.\n",
    "    \"\"\"\n",
    "    # training error\n",
    "    trainresults = pd.merge(submission_df[['ParcelId','201610','201611','201612']], comparison_df[['parcelid','logerror','month']],\n",
    "                           left_on='ParcelId', right_on='parcelid')\n",
    "    oct_error = abs(trainresults[trainresults['month'] == 10]['201610'] \n",
    "                    - trainresults[trainresults['month'] == 10]['logerror']).mean()\n",
    "    nov_error = abs(trainresults[trainresults['month'] == 11]['201611'] \n",
    "                    - trainresults[trainresults['month'] == 11]['logerror']).mean()\n",
    "    dec_error = abs(trainresults[trainresults['month'] == 12]['201612'] \n",
    "                    - trainresults[trainresults['month'] == 12]['logerror']).mean()\n",
    "    overall_months_mae = (oct_error*(trainresults['month'] == 10).sum() + nov_error*(trainresults['month'] == 11).sum() \n",
    "                        + dec_error*(trainresults['month'] == 12).sum()) / (trainresults['month'].isin([10,11,12])).sum()\n",
    "    \n",
    "    overall_mae = abs(trainresults['201612'] - trainresults['logerror']).mean()\n",
    "    return (oct_error, nov_error, dec_error, overall_months_mae, overall_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maindir = \"/home/anerdi/Desktop/Zillow\"\n",
    "\n",
    "logerror = pd.read_csv(maindir + \"/data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "logerror['weeknumber'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').isocalendar()[1])\n",
    "logerror['month'] = logerror['transactiondate'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').month)\n",
    "properties = pd.read_csv(maindir + \"/data/properties_2016.csv/properties_2016.csv\", usecols=['parcelid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join on parcel id\n",
    "data = pd.merge(properties,logerror[['parcelid','logerror','month']], on='parcelid')\n",
    "del logerror, properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in predictions from the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/anerdi/Desktop/Zillow/submissions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BART = pd.read_csv(\"BART_submission.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "Ridge = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_ridge.csv.gz\", compression=\"gzip\")\n",
    "rf_ridge = pd.read_csv(\"two_stage_stage1_rf_stage2_ridge.csv.gz\", compression=\"gzip\")\n",
    "logistic_ridge = pd.read_csv(\"two_stage_ridge.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "Enet = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_enet.csv.gz\", compression=\"gzip\")\n",
    "rf_enet = pd.read_csv(\"two_stage_stage1_rf_stage2_enet.csv.gz\")\n",
    "logistic_enet = pd.read_csv(\"two_stage_enet.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "Lasso = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_lasso.csv.gz\", compression=\"gzip\")\n",
    "rf_lasso = pd.read_csv(\"two_stage_stage1_rf_stage2_lasso.csv.gz\")\n",
    "logistic_lasso = pd.read_csv(\"two_stage_lasso.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "Huber = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_huber.csv.gz\", compression=\"gzip\")\n",
    "rf_huber = pd.read_csv(\"two_stage_stage1_rf_stage2_huber.csv.gz\")\n",
    "logistic_huber = pd.read_csv(\"two_stage_huber.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "LARM =  pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_larm.csv.gz\", compression=\"gzip\")\n",
    "rf_larm = pd.read_csv(\"two_stage_stage1_rf_stage2_larm.csv.gz\")\n",
    "logistic_larm = pd.read_csv(\"two_stage_larm.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "LME = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage_lme.csv.gz\", compression=\"gzip\")\n",
    "rf_lme = pd.read_csv(\"two_stage_stage1_rf_stage2_lme.csv.gz\", compression=\"gzip\")\n",
    "logistic_LME = pd.read_csv(\"two_stage_lme.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "Adaptive_LASSO =  pd.read_csv(\"Adp-lasso-af.gz\", compression=\"gzip\")\n",
    "\n",
    "RF = pd.read_csv(\"two_stage_stage1_stacked_annrfs_stage2_rf.csv.gz\", compression=\"gzip\")\n",
    "logistic_RF = pd.read_csv(\"two_stage_rf.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "RF_singlestage = pd.read_csv(\"RF_n100_maxfeat5_maxdepth8.gz\", compression=\"gzip\")\n",
    "RF_2 = pd.read_csv(\"RF_n100_maxfeat10_maxdepth20_extreme.gz\", compression=\"gzip\")\n",
    "\n",
    "XGB = pd.read_csv(\"two_stage_xgb.csv.gz\", compression=\"gzip\")\n",
    "XGB600 = pd.read_csv(\"XGB_600.gz\", compression=\"gzip\")\n",
    "XGB3000 = pd.read_csv(\"XGB_3000_RF.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Order for Stacked Model (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "                'stacked_rfs_ridge': rf_ridge,\n",
    "                'stacked_rfs_enet': rf_enet,\n",
    "                'stacked_rfs_lasso': rf_lasso,\n",
    "                'stacked_rfs_larm': rf_larm, \n",
    "                'stacked_rfs_huber': rf_huber,\n",
    "                'stacked_annrfs_ridge': Ridge,\n",
    "                'stacked_annrfs_enet': Enet, \n",
    "                'stacked_annrfs_lasso': Lasso,\n",
    "                'stacked_annrfs_larm': LARM, \n",
    "                'stacked_annrfs_huber': Huber, \n",
    "                'logistic_ridge': logistic_ridge,\n",
    "                'logistic_enet': logistic_enet,\n",
    "                'logistic_lasso': logistic_lasso,\n",
    "                'logistic_larm': logistic_larm,\n",
    "                'logistic_huber': logistic_huber,\n",
    "#                 'stacked_rfs_rf', \n",
    "#                 'stacked_rfs_rf_overfit', \n",
    "                'stacked_annrfs_rf': RF,\n",
    "#                 'stacked_annrfs_rf_overfit', \n",
    "                'logistic_rf': logistic_RF, \n",
    "#                 'logistic_rf_overfit':RF_2,\n",
    "#                 'stacked_rfs_xgb600', \n",
    "#                 'stacked_annrfs_xgb600', \n",
    "#                 'logistic_xgb600':XGB\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_ridge',\n",
       " 'stacked_rfs_ridge',\n",
       " 'stacked_annrfs_ridge',\n",
       " 'logistic_enet',\n",
       " 'stacked_rfs_enet',\n",
       " 'stacked_annrfs_enet',\n",
       " 'logistic_lasso',\n",
       " 'stacked_rfs_lasso',\n",
       " 'stacked_annrfs_lasso',\n",
       " 'logistic_larm',\n",
       " 'stacked_rfs_larm',\n",
       " 'stacked_annrfs_larm',\n",
       " 'logistic_huber',\n",
       " 'stacked_rfs_huber',\n",
       " 'stacked_annrfs_huber',\n",
       " 'logistic_rf',\n",
       " 'stacked_annrfs_rf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_of_models = [\n",
    "        'logistic_ridge', 'stacked_rfs_ridge', 'stacked_annrfs_ridge',\n",
    "        'logistic_enet', 'stacked_rfs_enet', 'stacked_annrfs_enet',\n",
    "        'logistic_lasso', 'stacked_rfs_lasso', 'stacked_annrfs_lasso',\n",
    "        'logistic_larm', 'stacked_rfs_larm', 'stacked_annrfs_larm',\n",
    "        'logistic_huber', 'stacked_rfs_huber', 'stacked_annrfs_huber',\n",
    "        'logistic_rf', 'stacked_annrfs_rf',\n",
    "    ]\n",
    "\n",
    "\n",
    "order_of_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_weights = pd.read_csv(\"/home/anerdi/Desktop/Zillow/levelonedata/super_learner_weights_2.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npatterns = 2985217 # of test observations\n",
    "nmodels = len(order_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked_final = Ridge[['ParcelId']].copy()\n",
    "\n",
    "for col in ['201610','201611','201612']:\n",
    "    month_preds = np.zeros(npatterns)\n",
    "    X = np.zeros((npatterns,nmodels))\n",
    "    for i,model_name in enumerate(order_of_models):\n",
    "        X[:,i] = model_dict[model_name][col]\n",
    "\n",
    "    # make predictions on X in chunks of 1M patterns at a time\n",
    "    for j in range(30):\n",
    "        A = X[j*100000:(j+1)*100000,:]\n",
    "        B = test_weights.values[j*100000:(j+1)*100000,:].T\n",
    "        month_preds[j*100000:(j+1)*100000] = np.einsum('ij,ji->i', A, B)\n",
    "    # fencepost\n",
    "    month_preds[2900000:] = np.einsum('ij,ji->i',X[2900000:,:], test_weights.values[2900000:,:].T)\n",
    "    stacked_final = pd.concat([stacked_final, Series(month_preds, name=col)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_final['201710'] = 0\n",
    "stacked_final['201711'] = 0\n",
    "stacked_final['201712'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06198401966890031,\n",
       " 0.06125177848256764,\n",
       " 0.07355242281041613,\n",
       " 0.064182612581198673,\n",
       " 0.06687094848229157)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert all(stacked_final.ParcelId.unique() == Ridge.ParcelId.unique())\n",
    "mean_absolute_errors(stacked_final, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06168418150557012,\n",
       " 0.060981272449663664,\n",
       " 0.07328333524176578,\n",
       " 0.063895304944010647,\n",
       " 0.06659304264059372)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert all(stacked_final.ParcelId.unique() == Ridge.ParcelId.unique())\n",
    "mean_absolute_errors(stacked_final, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_final.to_csv(\"new_stacked.gz\", index=False, float_format='%.4g', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall_prop = 0.80\n",
    "\n",
    "# # stacking ridge\n",
    "# ridge_models = [logistic_ridge,rf_ridge,Ridge]\n",
    "# ridge_weights = [0.33127665,0.36622542,0.30249792]\n",
    "# ridge_win_prop = 0.18428136\n",
    "# ridge_combinations = [tuple([ridge_models[i],ridge_weights[i]*ridge_win_prop*overall_prop]) for i in range(len(ridge_models))]\n",
    "\n",
    "# # stacking enet\n",
    "# enet_models = [logistic_enet,rf_enet,Enet]\n",
    "# enet_weights = [0.33258377,0.36457491,0.30284132]\n",
    "# enet_win_prop = 0.02802548\n",
    "# enet_combinations = [tuple([enet_models[i],enet_weights[i]*enet_win_prop*overall_prop]) for i in range(len(enet_models))]\n",
    "\n",
    "# # stacking lasso\n",
    "# lasso_models = [logistic_lasso,rf_lasso,Lasso]\n",
    "# lasso_weights = [0.332473,0.36443091,0.3030961]\n",
    "# lasso_win_prop = 0.03631127\n",
    "# lasso_combinations = [tuple([lasso_models[i],lasso_weights[i]*lasso_win_prop*overall_prop]) for i in range(len(lasso_models))]\n",
    "\n",
    "# # stacking larm\n",
    "# larm_models = [logistic_larm,rf_larm,LARM]\n",
    "# larm_weights = [0.3308668,0.36512877,0.30400443]\n",
    "# larm_win_prop = 0.19560233\n",
    "# larm_combinations = [tuple([larm_models[i],larm_weights[i]*larm_win_prop*overall_prop]) for i in range(len(larm_models))]\n",
    "\n",
    "# # stacking huber\n",
    "# huber_models = [logistic_huber,rf_huber,Huber]\n",
    "# huber_weights = [0.32970368,0.36645804,0.30383827]\n",
    "# huber_win_prop = 0.37547494\n",
    "# huber_combinations = [tuple([huber_models[i],huber_weights[i]*huber_win_prop*overall_prop]) for i in range(len(huber_models))]\n",
    "\n",
    "# # stacking rfs\n",
    "# rf_models = [RF,logistic_RF]\n",
    "# rf_weights = [0.554838,0.445162]\n",
    "# rf_win_prop = 0.18030462\n",
    "# rf_combinations = [tuple([rf_models[i],rf_weights[i]*rf_win_prop*overall_prop]) for i in range(len(rf_models))]\n",
    "\n",
    "# models = (ridge_combinations \n",
    "#           + enet_combinations \n",
    "#           + lasso_combinations \n",
    "#           + larm_combinations \n",
    "#           + huber_combinations \n",
    "#           + rf_combinations\n",
    "#           + [(LME, 0.30*0.09),(logistic_LME,0.33*0.09),(rf_lme,0.37*0.09),(XGB600,0.05),(XGB3000, 0.03),(RF_2, 0.03)]\n",
    "#          )\n",
    "\n",
    "\n",
    "# huber_combinations = [tuple([huber_models[i],huber_weights[i]*0.35]) for i in range(len(huber_models))]\n",
    "# ridge_combinations = [tuple([ridge_models[i],ridge_weights[i]*0.25]) for i in range(len(ridge_models))]\n",
    "# rf_combinations = [tuple([rf_models[i],rf_weights[i]*0.25]) for i in range(len(rf_models))]\n",
    "\n",
    "# models = (ridge_combinations + huber_combinations  + rf_combinations\n",
    "#           + [(LME, 0.30*0.04),(logistic_LME,0.33*0.04),(rf_lme,0.37*0.04),(XGB600,0.05),(XGB3000, 0.03),(RF_2, 0.03)]\n",
    "#          )\n",
    "\n",
    "# models = [\n",
    "#         logistic_ridge,rf_ridge,Ridge,\n",
    "#         logistic_enet,rf_enet,Enet,\n",
    "#         logistic_lasso,rf_lasso,Lasso,\n",
    "#         logistic_larm,rf_larm,LARM,\n",
    "#         logistic_huber,rf_huber,Huber,\n",
    "#         RF,logistic_RF\n",
    "#         ]\n",
    "\n",
    "# weights = [ 0.0548103 ,  0.07566879,  0.05380227,  0.00758793,  0.01014677,\n",
    "#         0.01029078,  0.00775408,  0.01422321,  0.01433398,  0.06360565,\n",
    "#         0.08515093,  0.04684575,  0.11919136,  0.15139297,  0.10489061,\n",
    "#         0.08592634,  0.09437829]\n",
    "\n",
    "# models = ([tuple([models[i],weights[i]*0.80]) for i in range(len(models))] +\n",
    "#     [(LME, 0.30*0.15),(logistic_LME,0.33*0.15),(rf_lme,0.37*0.15),\n",
    "#     (XGB3000, 0.03),(RF_2, 0.02)])\n",
    "\n",
    "\n",
    "# current best ensemble\n",
    "# models = [(Ridge,0.08),(logistic_ridge,0.03),(rf_ridge,0.03),\n",
    "#     (Enet,0.06),(logistic_enet,0.03),(rf_enet,0.03),\n",
    "#     (Lasso,0.06),(logistic_lasso,0.03),(rf_lasso,0.03),\n",
    "#     (Huber, 0.06),(logistic_huber,0.03),(rf_huber,0.03),\n",
    "#     (LARM,0.06),(logistic_larm,0.03),(rf_larm,0.03),\n",
    "#     (LME, 0.05),(logistic_LME,0.08),(rf_lme,0.03),\n",
    "#     (XGB600,0.09),\n",
    "#     (RF,0.10),\n",
    "#     (XGB3000, 0.02),\n",
    "#     (RF_2, 0.01)]\n",
    "\n",
    "models = [(stacked_final,0.70),\n",
    "    (LME, 0.05),(logistic_LME,0.08),(rf_lme,0.03),\n",
    "    (XGB600,0.09),\n",
    "    (XGB3000, 0.03),\n",
    "    (RF_2, 0.02)]\n",
    "\n",
    "sum([y for x,y in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = Ridge[['ParcelId']].copy()\n",
    "cols = ['201610','201611','201612','201710','201711','201712']\n",
    "foo = models[0][0][cols]*models[0][1]\n",
    "for pair in models[1:]:\n",
    "    model,wt = pair\n",
    "    foo = foo + model[cols]*wt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06163584488647778,\n",
       " 0.06093236582694411,\n",
       " 0.07323018976423241,\n",
       " 0.063845867478342311,\n",
       " 0.06654205594018466)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = pd.concat([ensemble,foo.round(4)], axis=1)\n",
    "ensemble['ParcelId'] = ensemble['ParcelId'].astype(int)\n",
    "assert all(ensemble.ParcelId.unique() == Ridge.ParcelId.unique())\n",
    "mean_absolute_errors(ensemble, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.062031906771147276,\n",
       " 0.061217579408543304,\n",
       " 0.07365583668775155,\n",
       " 0.06422425661437603)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best\n",
    "ensemble = pd.concat([ensemble,foo.round(4)], axis=1)\n",
    "ensemble['ParcelId'] = ensemble['ParcelId'].astype(int)\n",
    "assert all(ensemble.ParcelId.unique() == Ridge.ParcelId.unique())\n",
    "mean_absolute_errors(ensemble, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble.to_csv(\"new_ensemble.gz\", index=False, float_format='%.4g', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
